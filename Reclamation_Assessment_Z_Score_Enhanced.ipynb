{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLGmx3rVpNzY8QnHtFkW9e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header_readme"
      },
      "source": [
        "# Reclamation Assessment using Z-Score Transformed NDVI (Enhanced)\n",
        "\n",
        "## Overview\n",
        "This enhanced notebook analyzes crop growth within lease boundaries compared to the background field using z-score transformation of NDVI rasters. You can now choose between:\n",
        "- **Robust Z-Score**: Uses median and MAD (resistant to outliers)\n",
        "- **Normal Z-Score**: Uses mean and standard deviation (better for uniform data)\n",
        "\n",
        "## Key Features\n",
        "1. **Automatic Distribution Analysis**: Tests data normality and provides recommendations\n",
        "2. **Flexible Transformation**: Choose the method that best fits your data\n",
        "3. **Comparison Preview**: See both methods side-by-side before choosing\n",
        "4. **Consistent Processing**: Apply the same method across all years for fair comparison\n",
        "\n",
        "## Workflow\n",
        "1. Upload NDVI rasters and boundary polygons\n",
        "2. Analyze data distribution and review recommendations\n",
        "3. Choose z-score method (or accept recommendation)\n",
        "4. Process all rasters with selected method\n",
        "5. Download transformed rasters for GIS analysis\n",
        "\n",
        "## Formula Reference\n",
        "- **Robust Z-Score**: `Z = (NDVI - Median) / (1.4826 √ó MAD)`\n",
        "- **Normal Z-Score**: `Z = (NDVI - Mean) / Standard_Deviation`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_setup"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_imports"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q geopandas rasterio fiona shapely numpy pandas matplotlib ipywidgets\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import warnings\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from rasterio.mask import mask\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from shapely.geometry import shape, mapping\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import files\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Setup complete. All libraries imported successfully.\")\n",
        "print(\"üìç Running in Google Colab environment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_upload"
      },
      "source": [
        "## 2. File Upload Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_upload_init"
      },
      "outputs": [],
      "source": [
        "# Initialize file storage if not exists\n",
        "if 'all_uploaded_files' not in locals():\n",
        "    all_uploaded_files = {}\n",
        "if 'ndvi_files' not in locals():\n",
        "    ndvi_files = []\n",
        "if 'polygon_files' not in locals():\n",
        "    polygon_files = []\n",
        "\n",
        "print(\"üìÇ Upload your files (you can run this cell multiple times to add more):\")\n",
        "print(\"=\"*50)\n",
        "print(\"Required files:\")\n",
        "print(\"1Ô∏è‚É£ NDVI Rasters (.tif or .tiff files)\")\n",
        "print(\"2Ô∏è‚É£ Field Boundary polygon (.kml, .geojson, or .shp)\")\n",
        "print(\"3Ô∏è‚É£ Lease Boundary polygon (.kml, .geojson, or .shp)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Add to master collection and save to disk\n",
        "for filename, content in uploaded.items():\n",
        "    all_uploaded_files[filename] = content\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(content)\n",
        "    print(f\"‚úÖ Added: {filename} ({len(content)/1024:.1f} KB)\")\n",
        "\n",
        "# Re-categorize all files\n",
        "ndvi_files = []\n",
        "polygon_files = []\n",
        "\n",
        "for filename in all_uploaded_files.keys():\n",
        "    if filename.lower().endswith(('.tif', '.tiff')):\n",
        "        ndvi_files.append(filename)\n",
        "    elif filename.lower().endswith(('.kml', '.geojson', '.shp', '.json')):\n",
        "        polygon_files.append(filename)\n",
        "\n",
        "# Sort files for consistent processing\n",
        "ndvi_files.sort()\n",
        "polygon_files.sort()\n",
        "\n",
        "print(f\"\\nüìä Total Files Summary:\")\n",
        "print(f\"   - NDVI rasters: {len(ndvi_files)} files\")\n",
        "print(f\"   - Polygon files: {len(polygon_files)} files\")\n",
        "\n",
        "if len(ndvi_files) > 0 and len(polygon_files) >= 2:\n",
        "    print(\"\\n‚úÖ All required file types present!\")\n",
        "else:\n",
        "    if len(ndvi_files) == 0:\n",
        "        print(\"\\n‚ö†Ô∏è Warning: No NDVI rasters uploaded yet\")\n",
        "    if len(polygon_files) < 2:\n",
        "        print(f\"\\n‚ö†Ô∏è Warning: Need 2 polygon files (have {len(polygon_files)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_identify"
      },
      "source": [
        "## 3. Identify Field and Lease Boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_identify"
      },
      "outputs": [],
      "source": [
        "# Helper function to load polygon\n",
        "def load_polygon(filename: str) -> gpd.GeoDataFrame:\n",
        "    \"\"\"Load polygon from various formats\"\"\"\n",
        "    try:\n",
        "        gdf = gpd.read_file(filename)\n",
        "        return gdf\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load and identify polygons\n",
        "if len(polygon_files) >= 2:\n",
        "    print(\"üîç Identifying field and lease boundaries...\\n\")\n",
        "    \n",
        "    # Try to auto-identify based on filename\n",
        "    field_boundary_file = None\n",
        "    lease_boundary_file = None\n",
        "    \n",
        "    for filename in polygon_files:\n",
        "        fname_lower = filename.lower()\n",
        "        if 'field' in fname_lower and not field_boundary_file:\n",
        "            field_boundary_file = filename\n",
        "        elif 'lease' in fname_lower and not lease_boundary_file:\n",
        "            lease_boundary_file = filename\n",
        "    \n",
        "    # If not automatically identified, use manual selection\n",
        "    if not field_boundary_file or not lease_boundary_file:\n",
        "        field_boundary_file = polygon_files[0]\n",
        "        lease_boundary_file = polygon_files[1]\n",
        "    \n",
        "    print(f\"üìç Field Boundary: {field_boundary_file}\")\n",
        "    print(f\"üìç Lease Boundary: {lease_boundary_file}\")\n",
        "    \n",
        "    # Load the polygons\n",
        "    field_gdf = load_polygon(field_boundary_file)\n",
        "    lease_gdf = load_polygon(lease_boundary_file)\n",
        "    \n",
        "    if field_gdf is not None and lease_gdf is not None:\n",
        "        print(\"\\n‚úÖ Both boundaries loaded successfully\")\n",
        "else:\n",
        "    print(\"‚ùå Need at least 2 polygon files to proceed\")\n",
        "    field_gdf = None\n",
        "    lease_gdf = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_analysis"
      },
      "source": [
        "## 4. Analyze Data Distribution & Select Z-Score Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_stats_functions"
      },
      "outputs": [],
      "source": [
        "def calculate_all_stats(data: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Calculate both robust and normal statistics\"\"\"\n",
        "    # Remove NaN and NoData values\n",
        "    valid_data = data[~np.isnan(data)]\n",
        "    \n",
        "    # Additional filtering for common NoData values\n",
        "    nodata_values = [-9999, -10000, -3.4028235e+38, 3.4028235e+38]\n",
        "    for ndv in nodata_values:\n",
        "        valid_data = valid_data[np.abs(valid_data - ndv) > 1e-6]\n",
        "    \n",
        "    if len(valid_data) == 0:\n",
        "        return {\n",
        "            'median': np.nan, \n",
        "            'mad': np.nan, \n",
        "            'robust_std': np.nan,\n",
        "            'mean': np.nan,\n",
        "            'std': np.nan,\n",
        "            'n_valid': 0\n",
        "        }\n",
        "    \n",
        "    # Calculate robust statistics\n",
        "    median = np.median(valid_data)\n",
        "    mad = np.median(np.abs(valid_data - median))\n",
        "    robust_std = 1.4826 * mad  # Scale factor for consistency with standard deviation\n",
        "    \n",
        "    # Calculate normal statistics\n",
        "    mean = np.mean(valid_data)\n",
        "    std = np.std(valid_data)\n",
        "    \n",
        "    return {\n",
        "        'median': median,\n",
        "        'mad': mad,\n",
        "        'robust_std': robust_std,\n",
        "        'mean': mean,\n",
        "        'std': std,\n",
        "        'n_valid': len(valid_data)\n",
        "    }\n",
        "\n",
        "def test_distribution_normality(stats: Dict[str, float]) -> Dict[str, Any]:\n",
        "    \"\"\"Test if distribution is suitable for normal z-scores (rule of thumb)\"\"\"\n",
        "    \n",
        "    if stats['n_valid'] == 0 or np.isnan(stats['mean']):\n",
        "        return {\n",
        "            'recommendation': 'robust',\n",
        "            'reason': 'No valid data',\n",
        "            'details': {}\n",
        "        }\n",
        "    \n",
        "    # Test 1: Check if mean and median are close (|Œº ‚àí m| ‚â§ 0.2œÉ)\n",
        "    mean_median_diff = abs(stats['mean'] - stats['median'])\n",
        "    threshold = 0.2 * stats['std'] if stats['std'] > 0 else 0\n",
        "    condition1 = mean_median_diff <= threshold\n",
        "    \n",
        "    # Test 2: Check if MAD and std are proportional (0.8 ‚â§ MADs/œÉ ‚â§ 1.2)\n",
        "    if stats['std'] > 0:\n",
        "        ratio = stats['robust_std'] / stats['std']\n",
        "        condition2 = 0.8 <= ratio <= 1.2\n",
        "    else:\n",
        "        ratio = np.inf\n",
        "        condition2 = False\n",
        "    \n",
        "    # Test 3: Check for extreme uniformity (very small MAD)\n",
        "    if stats['mad'] < 0.005:  # Extremely uniform data\n",
        "        return {\n",
        "            'recommendation': 'normal',\n",
        "            'reason': '‚ö†Ô∏è HIGH UNIFORMITY DETECTED',\n",
        "            'warning': 'Data is extremely uniform - normal z-scores recommended',\n",
        "            'details': {\n",
        "                'mean_median_diff': mean_median_diff,\n",
        "                'threshold': threshold,\n",
        "                'mad_std_ratio': ratio,\n",
        "                'mad': stats['mad']\n",
        "            }\n",
        "        }\n",
        "    \n",
        "    # Determine recommendation\n",
        "    if condition1 and condition2:\n",
        "        return {\n",
        "            'recommendation': 'normal',\n",
        "            'reason': 'Distribution is approximately normal',\n",
        "            'details': {\n",
        "                'mean_median_diff': mean_median_diff,\n",
        "                'threshold': threshold,\n",
        "                'mad_std_ratio': ratio\n",
        "            }\n",
        "        }\n",
        "    else:\n",
        "        reasons = []\n",
        "        if not condition1:\n",
        "            reasons.append(f'Mean-median difference too large')\n",
        "        if not condition2:\n",
        "            reasons.append(f'MAD/Std ratio outside normal range')\n",
        "        \n",
        "        return {\n",
        "            'recommendation': 'robust',\n",
        "            'reason': 'Distribution appears skewed or contains outliers',\n",
        "            'details': {\n",
        "                'mean_median_diff': mean_median_diff,\n",
        "                'threshold': threshold,\n",
        "                'mad_std_ratio': ratio,\n",
        "                'issues': reasons\n",
        "            }\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Statistical analysis functions loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_analyze_data"
      },
      "outputs": [],
      "source": [
        "# Analyze each NDVI raster's distribution\n",
        "distribution_analysis = []\n",
        "\n",
        "if ndvi_files and field_gdf is not None and lease_gdf is not None:\n",
        "    print(\"üîç Analyzing data distributions...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    field_geom = field_gdf.geometry[0]\n",
        "    lease_geom = lease_gdf.geometry[0]\n",
        "    \n",
        "    for i, raster_file in enumerate(ndvi_files):\n",
        "        print(f\"\\n[{i+1}/{len(ndvi_files)}] {raster_file}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        try:\n",
        "            with rasterio.open(raster_file) as src:\n",
        "                # Reproject polygons if needed\n",
        "                raster_crs = src.crs\n",
        "                if field_gdf.crs != raster_crs:\n",
        "                    field_geom_proj = field_gdf.to_crs(raster_crs).geometry[0]\n",
        "                    lease_geom_proj = lease_gdf.to_crs(raster_crs).geometry[0]\n",
        "                else:\n",
        "                    field_geom_proj = field_geom\n",
        "                    lease_geom_proj = lease_geom\n",
        "                \n",
        "                # Get field data\n",
        "                field_data, out_transform = mask(src, [field_geom_proj], crop=True, nodata=np.nan, filled=True)\n",
        "                field_data = field_data[0].astype(np.float32)\n",
        "                \n",
        "                # Create masks\n",
        "                from rasterio.features import geometry_mask\n",
        "                field_mask = ~geometry_mask([field_geom_proj], out_shape=field_data.shape, transform=out_transform, invert=False)\n",
        "                lease_mask = ~geometry_mask([lease_geom_proj], out_shape=field_data.shape, transform=out_transform, invert=False)\n",
        "                lease_mask = lease_mask & field_mask\n",
        "                background_mask = field_mask & ~lease_mask\n",
        "                \n",
        "                # Extract background pixels\n",
        "                background_pixels = np.full_like(field_data, np.nan)\n",
        "                background_pixels[background_mask] = field_data[background_mask]\n",
        "                \n",
        "                # Calculate statistics\n",
        "                stats = calculate_all_stats(background_pixels)\n",
        "                test_result = test_distribution_normality(stats)\n",
        "                \n",
        "                # Store analysis\n",
        "                analysis = {\n",
        "                    'filename': raster_file,\n",
        "                    'stats': stats,\n",
        "                    'test': test_result,\n",
        "                    'field_data': field_data,\n",
        "                    'field_mask': field_mask,\n",
        "                    'lease_mask': lease_mask,\n",
        "                    'background_mask': background_mask,\n",
        "                    'transform': out_transform,\n",
        "                    'crs': raster_crs\n",
        "                }\n",
        "                distribution_analysis.append(analysis)\n",
        "                \n",
        "                # Display results\n",
        "                print(f\"üìä Background Statistics:\")\n",
        "                print(f\"   Mean: {stats['mean']:.4f}   |  Median: {stats['median']:.4f}\")\n",
        "                print(f\"   Std:  {stats['std']:.4f}   |  MAD:    {stats['mad']:.4f}\")\n",
        "                print(f\"   Robust Std: {stats['robust_std']:.4f}\")\n",
        "                print(f\"\\nüéØ Recommendation: {test_result['recommendation'].upper()} Z-SCORE\")\n",
        "                print(f\"   Reason: {test_result['reason']}\")\n",
        "                \n",
        "                if 'warning' in test_result:\n",
        "                    print(f\"   ‚ö†Ô∏è {test_result['warning']}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "    \n",
        "    # Summarize recommendations\n",
        "    recommendations = [a['test']['recommendation'] for a in distribution_analysis]\n",
        "    robust_count = recommendations.count('robust')\n",
        "    normal_count = recommendations.count('normal')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä DISTRIBUTION ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Files recommending ROBUST z-score: {robust_count}\")\n",
        "    print(f\"Files recommending NORMAL z-score: {normal_count}\")\n",
        "    \n",
        "    if robust_count > normal_count:\n",
        "        overall_recommendation = 'robust'\n",
        "        print(\"\\n‚úÖ Overall Recommendation: Use ROBUST Z-SCORE\")\n",
        "        print(\"   (Better for skewed data and outliers)\")\n",
        "    elif normal_count > robust_count:\n",
        "        overall_recommendation = 'normal'\n",
        "        print(\"\\n‚úÖ Overall Recommendation: Use NORMAL Z-SCORE\")\n",
        "        print(\"   (Better for uniform, normally-distributed data)\")\n",
        "    else:\n",
        "        overall_recommendation = 'robust'\n",
        "        print(\"\\n‚úÖ Overall Recommendation: Use ROBUST Z-SCORE\")\n",
        "        print(\"   (Safer default when recommendations are mixed)\")\n",
        "    \n",
        "    print(\"\\nüí° Important: Apply the same method to all years for consistency\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot analyze: Missing required files\")\n",
        "    distribution_analysis = []\n",
        "    overall_recommendation = 'robust'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_choice"
      },
      "source": [
        "## 5. Choose Z-Score Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_user_choice"
      },
      "outputs": [],
      "source": [
        "# Create interactive selection widget\n",
        "print(\"üéØ SELECT Z-SCORE TRANSFORMATION METHOD\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create radio buttons for selection\n",
        "method_selector = widgets.RadioButtons(\n",
        "    options=[\n",
        "        ('Robust Z-Score (Median & MAD) - Resistant to outliers', 'robust'),\n",
        "        ('Normal Z-Score (Mean & Std Dev) - Better for uniform data', 'normal'),\n",
        "        (f'Follow Recommendation ({overall_recommendation.upper()})', overall_recommendation)\n",
        "    ],\n",
        "    value=overall_recommendation,\n",
        "    description='Method:',\n",
        "    disabled=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Display comparison table\n",
        "comparison_html = \"\"\"\n",
        "<table style=\"width:100%; border-collapse: collapse; margin: 10px 0;\">\n",
        "<tr style=\"background-color: #f0f0f0;\">\n",
        "    <th style=\"padding: 8px; border: 1px solid #ddd;\">Aspect</th>\n",
        "    <th style=\"padding: 8px; border: 1px solid #ddd;\">Robust Z-Score</th>\n",
        "    <th style=\"padding: 8px; border: 1px solid #ddd;\">Normal Z-Score</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\"><b>Formula</b></td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Z = (x - Median) / (1.4826 √ó MAD)</td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Z = (x - Mean) / Std Dev</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\"><b>Best For</b></td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Skewed data, outliers present</td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Normal distribution, uniform data</td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\"><b>Sensitivity</b></td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">Less sensitive to extreme values</td>\n",
        "    <td style=\"padding: 8px; border: 1px solid #ddd;\">More sensitive to all values</td>\n",
        "</tr>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(comparison_html))\n",
        "display(method_selector)\n",
        "\n",
        "# Store the selected method\n",
        "selected_method = method_selector.value\n",
        "\n",
        "print(\"\\nüìå Note: The same method will be applied to all rasters for consistency\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_process"
      },
      "source": [
        "## 6. Process NDVI Rasters with Selected Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_process_rasters"
      },
      "outputs": [],
      "source": [
        "def process_with_selected_method(analysis: Dict, method: str) -> Dict[str, Any]:\n",
        "    \"\"\"Process a raster with the selected z-score method\"\"\"\n",
        "    \n",
        "    field_data = analysis['field_data']\n",
        "    field_mask = analysis['field_mask']\n",
        "    lease_mask = analysis['lease_mask']\n",
        "    stats = analysis['stats']\n",
        "    \n",
        "    # Initialize z-score raster with NaN\n",
        "    z_score_raster = np.full(field_data.shape, np.nan, dtype=np.float32)\n",
        "    \n",
        "    # Find valid field pixels\n",
        "    valid_field_pixels = field_mask & ~np.isnan(field_data)\n",
        "    nodata_values = [-9999, -10000, -3.4028235e+38]\n",
        "    for ndv in nodata_values:\n",
        "        valid_field_pixels = valid_field_pixels & (np.abs(field_data - ndv) > 1e-6)\n",
        "    \n",
        "    # Calculate z-scores based on selected method\n",
        "    if method == 'robust':\n",
        "        if stats['robust_std'] > 0:\n",
        "            z_score_raster[valid_field_pixels] = (\n",
        "                field_data[valid_field_pixels] - stats['median']\n",
        "            ) / stats['robust_std']\n",
        "        center_value = stats['median']\n",
        "        spread_value = stats['robust_std']\n",
        "        method_label = 'Robust Z-Score (Median & MAD)'\n",
        "    else:  # normal\n",
        "        if stats['std'] > 0:\n",
        "            z_score_raster[valid_field_pixels] = (\n",
        "                field_data[valid_field_pixels] - stats['mean']\n",
        "            ) / stats['std']\n",
        "        center_value = stats['mean']\n",
        "        spread_value = stats['std']\n",
        "        method_label = 'Normal Z-Score (Mean & Std Dev)'\n",
        "    \n",
        "    # Ensure everything outside field boundary is NaN\n",
        "    z_score_raster[~field_mask] = np.nan\n",
        "    \n",
        "    # Calculate lease statistics\n",
        "    lease_pixels = z_score_raster[lease_mask & ~np.isnan(z_score_raster)]\n",
        "    \n",
        "    if len(lease_pixels) > 0:\n",
        "        lease_stats = {\n",
        "            'mean_z': np.mean(lease_pixels),\n",
        "            'median_z': np.median(lease_pixels),\n",
        "            'std_z': np.std(lease_pixels),\n",
        "            'min_z': np.min(lease_pixels),\n",
        "            'max_z': np.max(lease_pixels),\n",
        "            'n_pixels': len(lease_pixels),\n",
        "            'pct_above_zero': np.sum(lease_pixels > 0) / len(lease_pixels) * 100,\n",
        "            'pct_below_minus2': np.sum(lease_pixels < -2) / len(lease_pixels) * 100,\n",
        "            'pct_above_2': np.sum(lease_pixels > 2) / len(lease_pixels) * 100\n",
        "        }\n",
        "    else:\n",
        "        lease_stats = None\n",
        "    \n",
        "    return {\n",
        "        'filename': analysis['filename'],\n",
        "        'z_score_raster': z_score_raster,\n",
        "        'transform': analysis['transform'],\n",
        "        'crs': analysis['crs'],\n",
        "        'stats': stats,\n",
        "        'lease_stats': lease_stats,\n",
        "        'method': method,\n",
        "        'method_label': method_label,\n",
        "        'center_value': center_value,\n",
        "        'spread_value': spread_value,\n",
        "        'lease_mask': lease_mask,\n",
        "        'field_mask': field_mask,\n",
        "        'background_mask': analysis['background_mask'],\n",
        "        'shape': z_score_raster.shape,\n",
        "        'success': True\n",
        "    }\n",
        "\n",
        "# Process all rasters with selected method\n",
        "processed_rasters = []\n",
        "selected_method = method_selector.value\n",
        "\n",
        "if distribution_analysis:\n",
        "    print(f\"\\nüîÑ Processing with {selected_method.upper()} Z-SCORE method...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, analysis in enumerate(distribution_analysis):\n",
        "        print(f\"\\n[{i+1}/{len(distribution_analysis)}] {analysis['filename']}\")\n",
        "        \n",
        "        result = process_with_selected_method(analysis, selected_method)\n",
        "        processed_rasters.append(result)\n",
        "        \n",
        "        print(f\"‚úÖ Processed using {result['method_label']}\")\n",
        "        print(f\"   Center: {result['center_value']:.4f}\")\n",
        "        print(f\"   Spread: {result['spread_value']:.4f}\")\n",
        "        \n",
        "        if result['lease_stats']:\n",
        "            lease_stats = result['lease_stats']\n",
        "            print(f\"\\n   Lease Area Z-Scores:\")\n",
        "            print(f\"   ‚Ä¢ Mean: {lease_stats['mean_z']:.3f}\")\n",
        "            print(f\"   ‚Ä¢ Above background (Z>0): {lease_stats['pct_above_zero']:.1f}%\")\n",
        "            \n",
        "            # Interpretation\n",
        "            if lease_stats['mean_z'] > 0.5:\n",
        "                print(\"   ‚úÖ Lease performing ABOVE background\")\n",
        "            elif lease_stats['mean_z'] > -0.5:\n",
        "                print(\"   ‚ö†Ô∏è Lease performing SIMILAR to background\")\n",
        "            else:\n",
        "                print(\"   ‚ùå Lease performing BELOW background\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processed {len(processed_rasters)} rasters with {selected_method.upper()} method\")\n",
        "else:\n",
        "    print(\"‚ùå No data to process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_visualize"
      },
      "source": [
        "## 7. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_visualize"
      },
      "outputs": [],
      "source": [
        "def plot_z_score_results(result: Dict, figsize=(15, 8)):\n",
        "    \"\"\"Create comprehensive visualization of z-score results\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "    \n",
        "    z_raster = result['z_score_raster']\n",
        "    lease_mask = result['lease_mask']\n",
        "    field_mask = result['field_mask']\n",
        "    background_mask = result['background_mask']\n",
        "    \n",
        "    # Create custom colormap\n",
        "    colors = ['darkred', 'red', 'white', 'lightgreen', 'darkgreen']\n",
        "    cmap = mcolors.LinearSegmentedColormap.from_list('z_score', colors, N=100)\n",
        "    vmin, vmax = -3, 3\n",
        "    \n",
        "    # Plot 1: Z-score map\n",
        "    im1 = axes[0].imshow(z_raster, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    axes[0].set_title(f'Z-Score Map\\n({result[\"method_label\"]})', fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Add lease boundary overlay\n",
        "    lease_overlay = np.ma.masked_where(~lease_mask, np.ones_like(z_raster))\n",
        "    axes[0].imshow(lease_overlay, alpha=0.2, cmap='Blues')\n",
        "    \n",
        "    # Plot 2: Area masks\n",
        "    mask_display = np.zeros_like(z_raster)\n",
        "    mask_display[background_mask] = 1\n",
        "    mask_display[lease_mask] = 2\n",
        "    mask_display[~field_mask] = np.nan\n",
        "    \n",
        "    cmap_masks = mcolors.ListedColormap(['lightgray', 'lightblue'])\n",
        "    axes[1].imshow(mask_display, cmap=cmap_masks, alpha=0.8)\n",
        "    axes[1].set_title('Area Definitions', fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='lightgray', label='Background'),\n",
        "        Patch(facecolor='lightblue', label='Lease Area')\n",
        "    ]\n",
        "    axes[1].legend(handles=legend_elements, loc='upper right')\n",
        "    \n",
        "    # Plot 3: Distribution histogram\n",
        "    lease_z = z_raster[lease_mask & ~np.isnan(z_raster)]\n",
        "    background_z = z_raster[background_mask & ~np.isnan(z_raster)]\n",
        "    \n",
        "    axes[2].hist(background_z, bins=50, alpha=0.5, label='Background', color='gray', density=True)\n",
        "    axes[2].hist(lease_z, bins=50, alpha=0.7, label='Lease Area', color='blue', density=True)\n",
        "    axes[2].axvline(0, color='black', linestyle='--', linewidth=2, label='Background Center')\n",
        "    axes[2].axvline(-2, color='red', linestyle=':', alpha=0.5)\n",
        "    axes[2].axvline(2, color='red', linestyle=':', alpha=0.5, label='¬±2 Std')\n",
        "    \n",
        "    axes[2].set_xlabel('Z-Score', fontsize=10)\n",
        "    axes[2].set_ylabel('Density', fontsize=10)\n",
        "    axes[2].set_title('Z-Score Distributions', fontweight='bold')\n",
        "    axes[2].legend(loc='upper right')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    axes[2].set_xlim(-4, 4)\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(im1, ax=axes, orientation='horizontal', pad=0.1, aspect=40)\n",
        "    cbar.set_label(f'Z-Score ({result[\"method\"].capitalize()} Method)', fontsize=10)\n",
        "    \n",
        "    # Add title\n",
        "    fig.suptitle(f\"File: {result['filename']}\", fontsize=13, fontweight='bold', y=1.02)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Visualize results\n",
        "if processed_rasters:\n",
        "    print(\"\\nüìä Generating visualizations...\")\n",
        "    \n",
        "    # Show first few rasters\n",
        "    max_plots = min(3, len(processed_rasters))\n",
        "    \n",
        "    for i in range(max_plots):\n",
        "        print(f\"\\nVisualization {i+1}/{max_plots}\")\n",
        "        fig = plot_z_score_results(processed_rasters[i])\n",
        "        plt.show()\n",
        "    \n",
        "    if len(processed_rasters) > max_plots:\n",
        "        print(f\"\\nüìå Showing first {max_plots} of {len(processed_rasters)} visualizations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section_export"
      },
      "source": [
        "## 8. Export Results & Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell_export"
      },
      "outputs": [],
      "source": [
        "# Save processed rasters and create download package\n",
        "if processed_rasters:\n",
        "    output_dir = f'zscore_{selected_method}_outputs'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"üíæ Saving {selected_method.upper()} z-score transformed rasters...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    saved_files = []\n",
        "    summary_data = []\n",
        "    \n",
        "    for i, result in enumerate(processed_rasters):\n",
        "        # Save GeoTIFF\n",
        "        base_name = os.path.splitext(result['filename'])[0]\n",
        "        output_file = os.path.join(output_dir, f\"{base_name}_zscore_{selected_method}.tif\")\n",
        "        \n",
        "        try:\n",
        "            z_data = result['z_score_raster'].copy()\n",
        "            \n",
        "            with rasterio.open(\n",
        "                output_file,\n",
        "                'w',\n",
        "                driver='GTiff',\n",
        "                height=result['shape'][0],\n",
        "                width=result['shape'][1],\n",
        "                count=1,\n",
        "                dtype='float32',\n",
        "                crs=result['crs'],\n",
        "                transform=result['transform'],\n",
        "                compress='lzw',\n",
        "                nodata=-9999\n",
        "            ) as dst:\n",
        "                z_data[np.isnan(z_data)] = -9999\n",
        "                dst.write(z_data.astype(np.float32), 1)\n",
        "                \n",
        "                # Add metadata\n",
        "                dst.update_tags(\n",
        "                    method=result['method'],\n",
        "                    center_value=str(result['center_value']),\n",
        "                    spread_value=str(result['spread_value']),\n",
        "                    processing_date=datetime.now().isoformat()\n",
        "                )\n",
        "            \n",
        "            saved_files.append(output_file)\n",
        "            print(f\"   ‚úÖ Saved: {os.path.basename(output_file)}\")\n",
        "            \n",
        "            # Collect summary data\n",
        "            row = {\n",
        "                'Filename': result['filename'],\n",
        "                'Method': result['method'],\n",
        "                'Center': result['center_value'],\n",
        "                'Spread': result['spread_value']\n",
        "            }\n",
        "            \n",
        "            if result['lease_stats']:\n",
        "                row.update({\n",
        "                    'Lease_Mean_Z': result['lease_stats']['mean_z'],\n",
        "                    'Lease_Pct_Above_Background': result['lease_stats']['pct_above_zero']\n",
        "                })\n",
        "            \n",
        "            summary_data.append(row)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error saving: {e}\")\n",
        "    \n",
        "    # Save summary CSV\n",
        "    df_summary = pd.DataFrame(summary_data)\n",
        "    summary_file = os.path.join(output_dir, f'zscore_{selected_method}_summary.csv')\n",
        "    df_summary.to_csv(summary_file, index=False)\n",
        "    \n",
        "    # Create ZIP archive\n",
        "    zip_filename = f'zscore_{selected_method}_results.zip'\n",
        "    \n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file in saved_files:\n",
        "            if os.path.exists(file):\n",
        "                zipf.write(file, os.path.basename(file))\n",
        "        \n",
        "        if os.path.exists(summary_file):\n",
        "            zipf.write(summary_file, os.path.basename(summary_file))\n",
        "        \n",
        "        # Add README with styling guide\n",
        "        readme_content = f\"\"\"Z-Score Transformed NDVI - {selected_method.upper()} Method\n",
        "=\"*60\n",
        "\n",
        "Processing Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
        "Method Used: {selected_method.upper()} Z-SCORE\n",
        "\n",
        "QGIS STYLING RECOMMENDATIONS\n",
        "-----------------------------\n",
        "\n",
        "For Individual Year Z-Scores (5 classes):\n",
        "1. Class 1: < -2 (Dark Red) - Significantly below background\n",
        "2. Class 2: -2 to -1 (Light Red) - Moderately below background\n",
        "3. Class 3: -1 to 0 (Yellow/White) - Slightly below background\n",
        "4. Class 4: 0 to 1 (Light Green) - Slightly above background\n",
        "5. Class 5: > 1 (Dark Green) - Above background\n",
        "\n",
        "For Multi-Year Composite Analysis:\n",
        "- If seeing saturation on negatives, try:\n",
        "  ‚Ä¢ Adjust min value to -5 or -10 instead of default -3\n",
        "  ‚Ä¢ Use percentile-based classification (2%, 25%, 50%, 75%, 98%)\n",
        "  ‚Ä¢ Apply histogram equalization for better contrast\n",
        "\n",
        "NoData Value: -9999\n",
        "\n",
        "Files included:\n",
        "- *_zscore_{selected_method}.tif: Transformed rasters\n",
        "- zscore_{selected_method}_summary.csv: Statistics summary\n",
        "\"\"\"\n",
        "        \n",
        "        zipf.writestr('README.txt', readme_content)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Archive created: {zip_filename}\")\n",
        "    print(\"\\n‚¨áÔ∏è Starting download...\")\n",
        "    \n",
        "    # Trigger download\n",
        "    files.download(zip_filename)\n",
        "    \n",
        "    print(\"\\nüéâ Processing complete!\")\n",
        "    print(f\"   Method used: {selected_method.upper()} Z-SCORE\")\n",
        "    print(f\"   Files processed: {len(processed_rasters)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No files to download\")"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawrencejesse/Sentinel2_Extractor/blob/main/Multi_Year_ProductivityRaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3lAj8Npw606"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb435f8e"
      },
      "source": [
        "# Task\n",
        "Create a Python notebook that allows the user to upload a polygon (KML/GeoJSON/SHP) and multiple NDVI rasters (GeoTIFF). The notebook should then:\n",
        "1. Load the uploaded data.\n",
        "2. Reproject and align all NDVI rasters to the grid of the first raster.\n",
        "3. Clip the rasters to the polygon boundary.\n",
        "4. Compute the mean NDVI within the polygon for each raster.\n",
        "5. Calculate a per-year Productivity Index for each raster using the formula (NDVI / mean) * 100.\n",
        "6. Calculate a Multi-Year Composite Productivity Index by averaging the per-year indices.\n",
        "7. Create a 3-class map based on the Composite Productivity Index (below average, average, above average).\n",
        "8. Save the per-year Productivity GeoTIFFs, the Composite GeoTIFF, the 3-class map GeoTIFF, and a CSV summary of the mean NDVI and productivity indices to a new folder in the notebook's memory.\n",
        "The notebook should be broken down into discrete steps for easy troubleshooting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "223b72ab"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Subtask:\n",
        "Install necessary libraries (geopandas, rasterio, etc.) and create a directory for outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7a633e"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the necessary libraries using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9f3ecc9",
        "outputId": "14b6b062-4cfd-4580-cbd4-44280ca1b223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting fiona\n",
            "  Downloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas) (25.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (2.2.2)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2025.10.5)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from rasterio) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->geopandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.17.0)\n",
            "Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio, fiona\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 fiona-1.10.1 rasterio-1.4.3\n"
          ]
        }
      ],
      "source": [
        "%pip install geopandas rasterio fiona shapely numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "346faf34"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries and create the output directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b328530",
        "outputId": "6ab44401-cfa6-4267-8f55-98c8da4ffb70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab environment\n"
          ]
        }
      ],
      "source": [
        "# Imports for both Jupyter and Colab environments\n",
        "%pip install -q geopandas rasterio fiona shapely numpy\n",
        "\n",
        "import geopandas\n",
        "import rasterio\n",
        "import rasterio.warp\n",
        "import rasterio.mask\n",
        "import fiona\n",
        "from shapely.geometry import shape\n",
        "import numpy as np\n",
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Detect environment (Colab vs Jupyter)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab environment\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running in Jupyter environment\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b282bdc"
      },
      "source": [
        "## Upload data\n",
        "\n",
        "### Subtask:\n",
        "Create widgets for uploading the AOI polygon and multiple NDVI rasters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75e1f1be"
      },
      "source": [
        "**Reasoning**:\n",
        "Create and display the file upload widgets for the AOI polygon and NDVI rasters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "feaeed66968742119f8a3b2bc1998c2b",
            "7ea562d20821420b8746958d37865d06",
            "3d06a8e3820042859bb724bc5a02c35a"
          ]
        },
        "id": "aYx9ZjUmKuat",
        "outputId": "31ea3e8f-94d0-44c3-ebb6-3e691e5412c5"
      },
      "source": [
        "# Create configuration widgets\n",
        "skip_alignment_widget = widgets.Checkbox(\n",
        "    value=False,\n",
        "    description='Skip Alignment',\n",
        "    tooltip='Skip the reprojection and alignment step if rasters are already aligned'\n",
        ")\n",
        "\n",
        "display(skip_alignment_widget)\n",
        "print(\"Configuration widgets created.\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Checkbox(value=False, description='Skip Alignment')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feaeed66968742119f8a3b2bc1998c2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration widgets created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "8f4b7505dfaa48d4b550931a399f46a6",
            "02f4a45ab3964926bac3ca9becc03ab4",
            "543382e49d1148c1a5bf07b68974d895",
            "447803c799c1492bb48950a4f42c77d2",
            "4b850c07f8c9434f9343ddf37306361a",
            "8b033930ff1341a49764f9ce97432255"
          ]
        },
        "id": "80b4289e",
        "outputId": "7d9a32bb-57b5-49ea-e821-42810da41521"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.kml,.geojson,.shp', description='Upload AOI Polygon (KML, GeoJSON, or SHP)')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f4b7505dfaa48d4b550931a399f46a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.tif,.tiff', description='Upload NDVI Rasters (GeoTIFFs)', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "447803c799c1492bb48950a4f42c77d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File upload widgets created. Please upload your AOI polygon and NDVI rasters.\n"
          ]
        }
      ],
      "source": [
        "# Create file upload widgets\n",
        "aoi_upload = widgets.FileUpload(\n",
        "    accept='.kml,.geojson,.shp',\n",
        "    multiple=False,\n",
        "    description='Upload AOI Polygon (KML, GeoJSON, or SHP)'\n",
        ")\n",
        "\n",
        "ndvi_uploads = widgets.FileUpload(\n",
        "    accept='.tif,.tiff',\n",
        "    multiple=True,\n",
        "    description='Upload NDVI Rasters (GeoTIFFs)'\n",
        ")\n",
        "\n",
        "display(aoi_upload)\n",
        "display(ndvi_uploads)\n",
        "print(\"File upload widgets created. Please upload your AOI polygon and NDVI rasters.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "366df83c"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the uploaded polygon and raster files into appropriate data structures (geopandas DataFrame and rasterio datasets).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91f68df7"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the uploaded AOI polygon and NDVI raster files into appropriate data structures using fiona and rasterio, handling cases where no files are uploaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9c873bc",
        "outputId": "aedb65c9-f893-45a2-a0cf-6fe24e5e5cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AOI polygon 'NW-21-2-14w1FieldBoundary.kml' loaded successfully.\n",
            "AOI CRS: EPSG:4326\n",
            "NDVI raster 'NW-21-002-14W1M_2019-07-25_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2019-07-30_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2019-08-19_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2020-07-14_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2021-07-22_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2021-07-27_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2021-08-08_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2021-08-13_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2022-07-17_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2022-08-08_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2023-07-22_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2024-07-28_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2024-08-02_NDVI.tif' loaded successfully.\n",
            "NDVI raster 'NW-21-002-14W1M_2025-07-13_NDVI.tif' loaded successfully.\n",
            "\n",
            "Summary: Loaded 14 NDVI raster(s) and 1 AOI polygon.\n"
          ]
        }
      ],
      "source": [
        "aoi_gdf = None\n",
        "original_ndvi_datasets = []\n",
        "memfiles = []  # Keep MemoryFile objects alive\n",
        "\n",
        "# Function to handle different widget structures (Jupyter vs Colab)\n",
        "def get_file_content(upload_widget, is_multiple=False):\n",
        "    \"\"\"Extract file content from upload widget, handling both Jupyter and Colab formats\"\"\"\n",
        "    files_data = []\n",
        "\n",
        "    if upload_widget.value:\n",
        "        # Check if it's Colab format (dict) or Jupyter format (tuple)\n",
        "        if isinstance(upload_widget.value, dict):\n",
        "            # Colab format: {'filename': {'content': bytes}}\n",
        "            for filename, file_info in upload_widget.value.items():\n",
        "                files_data.append({\n",
        "                    'name': filename,\n",
        "                    'content': file_info['content']\n",
        "                })\n",
        "        else:\n",
        "            # Jupyter format: ({'name': str, 'content': bytes}, ...)\n",
        "            for file_dict in upload_widget.value:\n",
        "                files_data.append({\n",
        "                    'name': file_dict['name'],\n",
        "                    'content': file_dict['content']\n",
        "                })\n",
        "\n",
        "    return files_data[0] if files_data and not is_multiple else files_data\n",
        "\n",
        "# Load AOI polygon\n",
        "aoi_file = get_file_content(aoi_upload, is_multiple=False)\n",
        "if aoi_file:\n",
        "    try:\n",
        "        with fiona.io.MemoryFile(aoi_file['content']) as memfile:\n",
        "            aoi_gdf = geopandas.read_file(memfile)\n",
        "        print(f\"AOI polygon '{aoi_file['name']}' loaded successfully.\")\n",
        "        print(f\"AOI CRS: {aoi_gdf.crs}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading AOI file: {e}\")\n",
        "else:\n",
        "    print(\"No AOI file uploaded. Skipping AOI loading.\")\n",
        "\n",
        "# Load NDVI rasters\n",
        "ndvi_files = get_file_content(ndvi_uploads, is_multiple=True)\n",
        "if ndvi_files:\n",
        "    for file_data in ndvi_files:\n",
        "        try:\n",
        "            # Keep MemoryFile alive by storing it\n",
        "            memfile = rasterio.io.MemoryFile(file_data['content'])\n",
        "            dataset = memfile.open()\n",
        "            original_ndvi_datasets.append(dataset)\n",
        "            memfiles.append(memfile)  # Important: Keep memfile alive!\n",
        "            print(f\"NDVI raster '{file_data['name']}' loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading NDVI file '{file_data['name']}': {e}\")\n",
        "else:\n",
        "    print(\"No NDVI files uploaded. Skipping NDVI loading.\")\n",
        "\n",
        "# Initialize ndvi_datasets for later use\n",
        "ndvi_datasets = []\n",
        "\n",
        "# Summary\n",
        "if aoi_gdf is not None and original_ndvi_datasets:\n",
        "    print(f\"\\nSummary: Loaded {len(original_ndvi_datasets)} NDVI raster(s) and 1 AOI polygon.\")\n",
        "elif original_ndvi_datasets:\n",
        "    print(f\"\\nSummary: Loaded {len(original_ndvi_datasets)} NDVI raster(s), no AOI polygon.\")\n",
        "elif aoi_gdf is not None:\n",
        "    print(f\"\\nSummary: Loaded 1 AOI polygon, no NDVI rasters.\")\n",
        "else:\n",
        "    print(f\"\\nSummary: No files loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0752f3a"
      },
      "source": [
        "## Reproject and align rasters\n",
        "\n",
        "### Subtask:\n",
        "Reproject and align all NDVI rasters to match the coordinate reference system and grid of the first raster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7c6460b"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to reproject and align NDVI rasters based on the instructions provided for the current subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa7e8379",
        "outputId": "049f5a51-8093-4181-927b-f25854ca730e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing reprojection and alignment...\n",
            "Target CRS: EPSG:4326\n",
            "Target dimensions: 247 x 166\n",
            "Dataset 0 (reference) read successfully.\n",
            "Dataset 1 reprojected and aligned.\n",
            "Dataset 2 reprojected and aligned.\n",
            "Dataset 3 reprojected and aligned.\n",
            "Dataset 4 reprojected and aligned.\n",
            "Dataset 5 reprojected and aligned.\n",
            "Dataset 6 reprojected and aligned.\n",
            "Dataset 7 reprojected and aligned.\n",
            "Dataset 8 reprojected and aligned.\n",
            "Dataset 9 reprojected and aligned.\n",
            "Dataset 10 reprojected and aligned.\n",
            "Dataset 11 reprojected and aligned.\n",
            "Dataset 12 reprojected and aligned.\n",
            "Dataset 13 reprojected and aligned.\n",
            "\n",
            "Alignment complete: 14 datasets ready for processing.\n"
          ]
        }
      ],
      "source": [
        "import rasterio.warp\n",
        "\n",
        "aligned_ndvi_arrays = []\n",
        "target_crs = None\n",
        "target_transform = None\n",
        "target_width = None\n",
        "target_height = None\n",
        "\n",
        "if not original_ndvi_datasets:\n",
        "    print(\"No NDVI datasets loaded. Skipping reprojection and alignment.\")\n",
        "    ndvi_datasets = original_ndvi_datasets\n",
        "else:\n",
        "    if skip_alignment_widget.value:\n",
        "        print(\"Skipping reprojection and alignment as requested.\")\n",
        "        # If alignment is skipped, use the original datasets for subsequent steps\n",
        "        ndvi_datasets = original_ndvi_datasets\n",
        "        # Get target CRS and transform from the first original dataset\n",
        "        if ndvi_datasets:\n",
        "            target_crs = ndvi_datasets[0].crs\n",
        "            target_transform = ndvi_datasets[0].transform\n",
        "            target_width = ndvi_datasets[0].width\n",
        "            target_height = ndvi_datasets[0].height\n",
        "            print(f\"Using CRS and transform from first dataset: {target_crs}\")\n",
        "    else:\n",
        "        print(\"Performing reprojection and alignment...\")\n",
        "        # Get CRS and transform from the first dataset\n",
        "        first_dataset = original_ndvi_datasets[0]\n",
        "        target_crs = first_dataset.crs\n",
        "        target_transform = first_dataset.transform\n",
        "        target_width = first_dataset.width\n",
        "        target_height = first_dataset.height\n",
        "\n",
        "        print(f\"Target CRS: {target_crs}\")\n",
        "        print(f\"Target dimensions: {target_width} x {target_height}\")\n",
        "\n",
        "        # Read all datasets and align them\n",
        "        for i, dataset in enumerate(original_ndvi_datasets):\n",
        "            try:\n",
        "                if i == 0:\n",
        "                    # First dataset sets the reference\n",
        "                    aligned_ndvi_arrays.append(dataset.read(1))\n",
        "                    print(f\"Dataset 0 (reference) read successfully.\")\n",
        "                else:\n",
        "                    # Reproject other datasets to match the first\n",
        "                    reprojected_data = np.empty((target_height, target_width), dtype=dataset.dtypes[0])\n",
        "\n",
        "                    rasterio.warp.reproject(\n",
        "                        source=rasterio.band(dataset, 1),\n",
        "                        destination=reprojected_data,\n",
        "                        src_transform=dataset.transform,\n",
        "                        src_crs=dataset.crs,\n",
        "                        dst_transform=target_transform,\n",
        "                        dst_crs=target_crs,\n",
        "                        resampling=rasterio.warp.Resampling.bilinear\n",
        "                    )\n",
        "                    aligned_ndvi_arrays.append(reprojected_data)\n",
        "                    print(f\"Dataset {i} reprojected and aligned.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing dataset {i}: {e}\")\n",
        "\n",
        "        # Use aligned arrays for subsequent steps\n",
        "        ndvi_datasets = aligned_ndvi_arrays\n",
        "        print(f\"\\nAlignment complete: {len(ndvi_datasets)} datasets ready for processing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4516710c"
      },
      "source": [
        "## Clip rasters to aoi\n",
        "\n",
        "### Subtask:\n",
        "Clip all aligned rasters to the boundaries of the uploaded AOI polygon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecf252fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if both aoi_gdf and ndvi_datasets are available, and if so, iterate through the aligned NDVI datasets and clip each one using the aoi_gdf geometry, storing the clipped data and transforms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b31a5ba",
        "outputId": "0ca9e4ec-bb8f-4970-a1a5-93bac671df72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDVI array 0 clipped successfully.\n",
            "NDVI array 1 clipped successfully.\n",
            "NDVI array 2 clipped successfully.\n",
            "NDVI array 3 clipped successfully.\n",
            "NDVI array 4 clipped successfully.\n",
            "NDVI array 5 clipped successfully.\n",
            "NDVI array 6 clipped successfully.\n",
            "NDVI array 7 clipped successfully.\n",
            "NDVI array 8 clipped successfully.\n",
            "NDVI array 9 clipped successfully.\n",
            "NDVI array 10 clipped successfully.\n",
            "NDVI array 11 clipped successfully.\n",
            "NDVI array 12 clipped successfully.\n",
            "NDVI array 13 clipped successfully.\n",
            "\n",
            "Clipping complete: 14 datasets clipped to AOI.\n"
          ]
        }
      ],
      "source": [
        "import rasterio.mask\n",
        "\n",
        "clipped_ndvi_data_and_transforms = []\n",
        "\n",
        "if aoi_gdf is None or not ndvi_datasets:\n",
        "    print(\"AOI polygon or NDVI datasets not available. Skipping clipping process.\")\n",
        "else:\n",
        "    if target_crs is None or target_transform is None:\n",
        "        print(\"Target CRS or Transform not defined. Cannot clip.\")\n",
        "    else:\n",
        "        # Ensure the AOI is in the same CRS as the rasters\n",
        "        if aoi_gdf.crs != target_crs:\n",
        "            try:\n",
        "                aoi_gdf = aoi_gdf.to_crs(target_crs)\n",
        "                print(f\"AOI polygon reprojected to target CRS: {target_crs}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reprojecting AOI polygon: {e}\")\n",
        "                aoi_gdf = None\n",
        "\n",
        "        if aoi_gdf is not None:\n",
        "            # Get the geometry for masking\n",
        "            geometries = aoi_gdf.geometry.values\n",
        "\n",
        "            for i, dataset_or_array in enumerate(ndvi_datasets):\n",
        "                try:\n",
        "                    if hasattr(dataset_or_array, 'read'):\n",
        "                        # It's a rasterio DatasetReader (when alignment was skipped)\n",
        "                        clipped_data, clipped_transform = rasterio.mask.mask(\n",
        "                            dataset_or_array, geometries, crop=True, nodata=np.nan\n",
        "                        )\n",
        "                        clipped_data = clipped_data[0]  # Get first band\n",
        "                        clipped_ndvi_data_and_transforms.append((clipped_data, clipped_transform))\n",
        "                        print(f\"NDVI dataset {i} clipped successfully.\")\n",
        "                    else:\n",
        "                        # It's a numpy array (when alignment was performed)\n",
        "                        # Create a temporary in-memory dataset for clipping\n",
        "                        height, width = dataset_or_array.shape\n",
        "                        dtype = dataset_or_array.dtype\n",
        "\n",
        "                        with rasterio.MemoryFile() as memfile:\n",
        "                            with memfile.open(\n",
        "                                driver='GTiff',\n",
        "                                height=height,\n",
        "                                width=width,\n",
        "                                count=1,\n",
        "                                dtype=dtype,\n",
        "                                crs=target_crs,\n",
        "                                transform=target_transform\n",
        "                            ) as tmp_dataset:\n",
        "                                tmp_dataset.write(dataset_or_array, 1)\n",
        "\n",
        "                                # Perform clipping\n",
        "                                clipped_data, clipped_transform = rasterio.mask.mask(\n",
        "                                    tmp_dataset, geometries, crop=True, nodata=np.nan\n",
        "                                )\n",
        "                                clipped_data = clipped_data[0]  # Get first band\n",
        "\n",
        "                        clipped_ndvi_data_and_transforms.append((clipped_data, clipped_transform))\n",
        "                        print(f\"NDVI array {i} clipped successfully.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error clipping item {i}: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "            # Replace ndvi_datasets with clipped data\n",
        "            ndvi_datasets = clipped_ndvi_data_and_transforms\n",
        "            print(f\"\\nClipping complete: {len(ndvi_datasets)} datasets clipped to AOI.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adef0b30"
      },
      "source": [
        "## Compute mean ndvi\n",
        "\n",
        "### Subtask:\n",
        "For each clipped raster, compute the mean NDVI value within the AOI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b44b3646"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if there are clipped NDVI datasets available before attempting to compute mean values. If not, print a message and finish the task. Otherwise, iterate through the clipped datasets, calculate the mean of each, store them, and print the list of means.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a913398d",
        "outputId": "9181413a-5e9e-46da-8629-97329208b8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean NDVI values for each clipped raster:\n",
            "[np.float32(0.79297763), np.float32(0.7953658), np.float32(0.5171646), np.float32(0.7105119), np.float32(0.7383192), np.float32(0.78750867), np.float32(0.70209044), np.float32(0.6723136), np.float32(0.86151296), np.float32(0.84641045), np.float32(0.7684731), np.float32(0.783863), np.float32(0.8419981), np.float32(0.82733035)]\n"
          ]
        }
      ],
      "source": [
        "mean_ndvi_values = []\n",
        "\n",
        "if not ndvi_datasets:\n",
        "    print(\"No clipped NDVI data available. Skipping mean NDVI computation.\")\n",
        "else:\n",
        "    # Assuming ndvi_datasets now contains tuples of (clipped_numpy_array, transform)\n",
        "    # from the previous clipping step.\n",
        "    # If the clipping step failed and ndvi_datasets was not updated,\n",
        "    # it might contain numpy arrays (the aligned data).\n",
        "    # Let's assume ndvi_datasets contains numpy arrays (either clipped or aligned).\n",
        "    # If it contains tuples, the code below will need adjustment to access the array part.\n",
        "    # Based on the previous step's output, ndvi_datasets was replaced with clipped_ndvi_datasets\n",
        "    # which are tuples.\n",
        "\n",
        "    # Let's double-check the structure of ndvi_datasets after clipping.\n",
        "    # If clipping was skipped, it might still hold the aligned numpy arrays.\n",
        "    # If clipping was attempted and successful, it should hold tuples (array, transform).\n",
        "\n",
        "    # To be safe, let's handle both cases.\n",
        "    if ndvi_datasets and isinstance(ndvi_datasets[0], tuple):\n",
        "        # Structure is (array, transform)\n",
        "        for clipped_data, _ in ndvi_datasets:\n",
        "            # Calculate the mean of the clipped data array\n",
        "            mean_value = np.nanmean(clipped_data) # Use nanmean to ignore potential NoData values\n",
        "            mean_ndvi_values.append(mean_value)\n",
        "    elif ndvi_datasets and isinstance(ndvi_datasets[0], np.ndarray):\n",
        "         # Structure is numpy array (clipping might have been skipped)\n",
        "         for data_array in ndvi_datasets:\n",
        "             # Calculate the mean of the data array\n",
        "             mean_value = np.nanmean(data_array) # Use nanmean to ignore potential NoData values\n",
        "             mean_ndvi_values.append(mean_value)\n",
        "    else:\n",
        "        print(\"Unexpected data structure in ndvi_datasets. Cannot compute mean NDVI.\")\n",
        "\n",
        "\n",
        "    if mean_ndvi_values:\n",
        "        print(\"Mean NDVI values for each clipped raster:\")\n",
        "        print(mean_ndvi_values)\n",
        "    else:\n",
        "        print(\"Mean NDVI computation resulted in an empty list.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b9a5e0"
      },
      "source": [
        "## Calculate per-year productivity index\n",
        "\n",
        "### Subtask:\n",
        "Calculate a per-year Productivity Index for each raster using the formula (NDVI / mean) * 100.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac53bfb"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if the necessary data (`ndvi_datasets` and `mean_ndvi_values`) is available and has consistent length. If so, iterate through the datasets and their corresponding mean values to calculate the per-year productivity index for each raster using the provided formula, handling potential division by zero, and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80be66da",
        "outputId": "4d00119e-d8cb-4694-cfb8-834a094a09ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated per-year productivity indices for 14 rasters out of 14.\n"
          ]
        }
      ],
      "source": [
        "per_year_productivity_indices = []\n",
        "mean_ndvi_values_recalc = []  # Recalculated means within AOI\n",
        "\n",
        "if not ndvi_datasets:\n",
        "    print(\"NDVI datasets are not available. Skipping per-year productivity index calculation.\")\n",
        "else:\n",
        "    print(f\"Processing {len(ndvi_datasets)} NDVI datasets...\")\n",
        "    \n",
        "    # Determine structure of ndvi_datasets\n",
        "    if ndvi_datasets and isinstance(ndvi_datasets[0], tuple):\n",
        "        # Structure is (array, transform) from clipping step\n",
        "        calculated_count = 0\n",
        "        \n",
        "        for i, (ndvi_data, transform) in enumerate(ndvi_datasets):\n",
        "            try:\n",
        "                # Calculate mean NDVI within the AOI (excluding NaN values)\n",
        "                valid_mask = ~np.isnan(ndvi_data)\n",
        "                \n",
        "                if np.any(valid_mask):\n",
        "                    mean_value = np.mean(ndvi_data[valid_mask])\n",
        "                    mean_ndvi_values_recalc.append(mean_value)\n",
        "                    \n",
        "                    # Check if mean is too close to zero (shouldn't happen with NDVI)\n",
        "                    if abs(mean_value) < 0.01:\n",
        "                        print(f\"Warning: Mean NDVI for raster {i} is very low ({mean_value:.4f}). This might indicate an issue with the data.\")\n",
        "                        # Still calculate the index\n",
        "                    \n",
        "                    # Calculate productivity index as ratio (not percentage)\n",
        "                    # Index = NDVI / mean, where 1.0 = field average\n",
        "                    productivity_index_array = np.full_like(ndvi_data, np.nan, dtype=np.float32)\n",
        "                    productivity_index_array[valid_mask] = ndvi_data[valid_mask] / mean_value\n",
        "                    \n",
        "                    per_year_productivity_indices.append(productivity_index_array)\n",
        "                    calculated_count += 1\n",
        "                    print(f\"  Raster {i+1}: Mean NDVI = {mean_value:.4f}, Index range = [{np.nanmin(productivity_index_array):.2f}, {np.nanmax(productivity_index_array):.2f}]\")\n",
        "                else:\n",
        "                    print(f\"  Raster {i+1}: No valid NDVI data within AOI\")\n",
        "                    mean_ndvi_values_recalc.append(np.nan)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating productivity index for raster {i}: {e}\")\n",
        "                mean_ndvi_values_recalc.append(np.nan)\n",
        "                \n",
        "    elif ndvi_datasets and isinstance(ndvi_datasets[0], np.ndarray):\n",
        "        # Structure is numpy array (alignment was done but no clipping)\n",
        "        calculated_count = 0\n",
        "        \n",
        "        for i, ndvi_data in enumerate(ndvi_datasets):\n",
        "            try:\n",
        "                # Calculate mean NDVI (excluding NaN values)\n",
        "                valid_mask = ~np.isnan(ndvi_data)\n",
        "                \n",
        "                if np.any(valid_mask):\n",
        "                    mean_value = np.mean(ndvi_data[valid_mask])\n",
        "                    mean_ndvi_values_recalc.append(mean_value)\n",
        "                    \n",
        "                    if abs(mean_value) < 0.01:\n",
        "                        print(f\"Warning: Mean NDVI for raster {i} is very low ({mean_value:.4f}).\")\n",
        "                    \n",
        "                    # Calculate productivity index as ratio\n",
        "                    productivity_index_array = np.full_like(ndvi_data, np.nan, dtype=np.float32)\n",
        "                    productivity_index_array[valid_mask] = ndvi_data[valid_mask] / mean_value\n",
        "                    \n",
        "                    per_year_productivity_indices.append(productivity_index_array)\n",
        "                    calculated_count += 1\n",
        "                    print(f\"  Raster {i+1}: Mean NDVI = {mean_value:.4f}, Index range = [{np.nanmin(productivity_index_array):.2f}, {np.nanmax(productivity_index_array):.2f}]\")\n",
        "                else:\n",
        "                    print(f\"  Raster {i+1}: No valid NDVI data\")\n",
        "                    mean_ndvi_values_recalc.append(np.nan)\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating productivity index for raster {i}: {e}\")\n",
        "                mean_ndvi_values_recalc.append(np.nan)\n",
        "    else:\n",
        "        print(\"Unexpected data structure in ndvi_datasets.\")\n",
        "    \n",
        "    if per_year_productivity_indices:\n",
        "        print(f\"\\nSuccessfully calculated {calculated_count} per-year productivity indices.\")\n",
        "        print(f\"Productivity index values are ratios where 1.0 = field average, >1.0 = above average, <1.0 = below average\")\n",
        "    else:\n",
        "        print(\"No productivity indices were calculated.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9b5f919"
      },
      "source": [
        "## Calculate multi-year composite productivity index\n",
        "\n",
        "### Subtask:\n",
        "Average the per-year Productivity Indices to create a Multi-Year Composite Productivity Index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8871f5b0"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if per-year productivity indices are available and calculate the multi-year composite index if they are.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa8f3648",
        "outputId": "beb3f415-3f44-4020-c276-85ada6ee2b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-Year Composite Productivity Index calculated.\n",
            "Shape of composite index: (83, 206)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3660887851.py:10: RuntimeWarning: Mean of empty slice\n",
            "  composite_productivity_index = np.nanmean(productivity_indices_3d, axis=0)\n"
          ]
        }
      ],
      "source": [
        "composite_productivity_index = None\n",
        "min_valid_observations = 2  # Require at least 2 valid observations per pixel\n",
        "\n",
        "if not per_year_productivity_indices:\n",
        "    print(\"No per-year productivity indices available. Skipping composite index calculation.\")\n",
        "else:\n",
        "    print(f\"Creating composite from {len(per_year_productivity_indices)} productivity indices...\")\n",
        "    \n",
        "    # Stack all indices into a 3D array (years, height, width)\n",
        "    productivity_indices_3d = np.stack(per_year_productivity_indices, axis=0)\n",
        "    \n",
        "    # Count valid observations per pixel\n",
        "    valid_count = np.sum(~np.isnan(productivity_indices_3d), axis=0)\n",
        "    \n",
        "    # Calculate mean along time axis (axis=0), ignoring NaN values\n",
        "    with np.errstate(invalid='ignore', divide='ignore'):\n",
        "        composite_productivity_index = np.nanmean(productivity_indices_3d, axis=0)\n",
        "    \n",
        "    # Mask out pixels with insufficient observations\n",
        "    insufficient_data_mask = valid_count < min_valid_observations\n",
        "    composite_productivity_index[insufficient_data_mask] = np.nan\n",
        "    \n",
        "    # Calculate statistics\n",
        "    valid_pixels = ~np.isnan(composite_productivity_index)\n",
        "    if np.any(valid_pixels):\n",
        "        composite_mean = np.mean(composite_productivity_index[valid_pixels])\n",
        "        composite_std = np.std(composite_productivity_index[valid_pixels])\n",
        "        composite_min = np.min(composite_productivity_index[valid_pixels])\n",
        "        composite_max = np.max(composite_productivity_index[valid_pixels])\n",
        "        \n",
        "        print(f\"Composite Productivity Index calculated successfully:\")\n",
        "        print(f\"  Shape: {composite_productivity_index.shape}\")\n",
        "        print(f\"  Valid pixels: {np.sum(valid_pixels)} / {composite_productivity_index.size}\")\n",
        "        print(f\"  Mean: {composite_mean:.3f} (should be close to 1.0)\")\n",
        "        print(f\"  Std Dev: {composite_std:.3f}\")\n",
        "        print(f\"  Range: [{composite_min:.3f}, {composite_max:.3f}]\")\n",
        "        print(f\"  Pixels with <{min_valid_observations} observations masked: {np.sum(insufficient_data_mask)}\")\n",
        "    else:\n",
        "        print(\"Warning: No valid pixels in composite index!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8cc4e44"
      },
      "source": [
        "## Create 3-class map\n",
        "\n",
        "### Subtask:\n",
        "Create a 3-class map based on the Composite Productivity Index (below average, average, above average).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce8aa325"
      },
      "source": [
        "**Reasoning**:\n",
        "Check if the composite productivity index is available. If it is, define the class boundaries and create the 3-class map based on these boundaries, handling potential NaN values. Finally, print the shape of the resulting map.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4503791",
        "outputId": "074471ef-f746-47bb-a0fd-4972b46c41a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3-class map created.\n",
            "Shape of 3-class map: (83, 206)\n"
          ]
        }
      ],
      "source": [
        "three_class_map = None\n",
        "zone_thresholds = {}\n",
        "\n",
        "if composite_productivity_index is None:\n",
        "    print(\"Composite productivity index not available. Skipping 3-class map creation.\")\n",
        "else:\n",
        "    print(\"Creating 3-zone productivity classification using quantile tertiles...\")\n",
        "    \n",
        "    # Get valid (non-NaN) values for quantile calculation\n",
        "    valid_values = composite_productivity_index[~np.isnan(composite_productivity_index)]\n",
        "    \n",
        "    if len(valid_values) > 0:\n",
        "        # Calculate tertile thresholds (33rd and 67th percentiles)\n",
        "        lower_threshold = np.percentile(valid_values, 33.33)\n",
        "        upper_threshold = np.percentile(valid_values, 66.67)\n",
        "        \n",
        "        zone_thresholds = {\n",
        "            'lower': lower_threshold,\n",
        "            'upper': upper_threshold,\n",
        "            'method': 'quantile_tertiles'\n",
        "        }\n",
        "        \n",
        "        # Create the 3-class map\n",
        "        three_class_map = np.full_like(composite_productivity_index, np.nan, dtype=np.float32)\n",
        "        \n",
        "        # Class 1: Low productivity (bottom tertile)\n",
        "        three_class_map[(composite_productivity_index <= lower_threshold)] = 1\n",
        "        \n",
        "        # Class 2: Medium productivity (middle tertile)\n",
        "        three_class_map[(composite_productivity_index > lower_threshold) & \n",
        "                       (composite_productivity_index <= upper_threshold)] = 2\n",
        "        \n",
        "        # Class 3: High productivity (top tertile)\n",
        "        three_class_map[composite_productivity_index > upper_threshold] = 3\n",
        "        \n",
        "        # Calculate zone statistics\n",
        "        zone_1_count = np.sum(three_class_map == 1)\n",
        "        zone_2_count = np.sum(three_class_map == 2)\n",
        "        zone_3_count = np.sum(three_class_map == 3)\n",
        "        total_classified = zone_1_count + zone_2_count + zone_3_count\n",
        "        \n",
        "        print(f\"3-zone classification complete:\")\n",
        "        print(f\"  Method: Quantile tertiles (equal area distribution)\")\n",
        "        print(f\"  Zone 1 (Low): \u2264 {lower_threshold:.3f} ({zone_1_count} pixels, {zone_1_count/total_classified*100:.1f}%)\")\n",
        "        print(f\"  Zone 2 (Medium): {lower_threshold:.3f} - {upper_threshold:.3f} ({zone_2_count} pixels, {zone_2_count/total_classified*100:.1f}%)\")\n",
        "        print(f\"  Zone 3 (High): > {upper_threshold:.3f} ({zone_3_count} pixels, {zone_3_count/total_classified*100:.1f}%)\")\n",
        "        print(f\"  Total classified pixels: {total_classified}\")\n",
        "    else:\n",
        "        print(\"No valid values in composite index. Cannot create 3-class map.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "096d8cc0"
      },
      "source": [
        "## Save outputs\n",
        "\n",
        "### Subtask:\n",
        "Save the per-year Productivity GeoTIFFs, the Composite GeoTIFF, the 3-class map GeoTIFF, and a CSV summary of the mean NDVI and productivity indices to a new folder in the notebook's memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47c76c0e"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the output directory if it doesn't exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4583baa5",
        "outputId": "2e56a3c2-b464-4a49-a0c4-5bc9bb47c8ea"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create output directory\n",
        "output_dir = 'productivity_outputs'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "    print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "# Check if required data is available\n",
        "if not per_year_productivity_indices or composite_productivity_index is None or three_class_map is None:\n",
        "    print(\"Missing required data. Please ensure all processing steps completed successfully.\")\n",
        "    print(f\"  Per-year indices available: {len(per_year_productivity_indices) if per_year_productivity_indices else 0}\")\n",
        "    print(f\"  Composite index available: {composite_productivity_index is not None}\")\n",
        "    print(f\"  Three-class map available: {three_class_map is not None}\")\n",
        "else:\n",
        "    print(f\"Saving all outputs to '{output_dir}/'...\")\n",
        "    \n",
        "    # Determine the transform and CRS to use for saving\n",
        "    save_transform = None\n",
        "    save_crs = None\n",
        "    \n",
        "    # Get transform and CRS from clipped data if available\n",
        "    if ndvi_datasets and isinstance(ndvi_datasets[0], tuple):\n",
        "        _, save_transform = ndvi_datasets[0]\n",
        "        save_crs = target_crs if 'target_crs' in locals() else None\n",
        "    elif 'target_transform' in locals() and 'target_crs' in locals():\n",
        "        save_transform = target_transform\n",
        "        save_crs = target_crs\n",
        "    \n",
        "    if save_transform is None or save_crs is None:\n",
        "        print(\"Warning: Georeferencing information not available. Saving with default transform.\")\n",
        "        # Create a default transform\n",
        "        height, width = composite_productivity_index.shape\n",
        "        save_transform = rasterio.transform.from_bounds(0, 0, width, height, width, height)\n",
        "        save_crs = 'EPSG:4326'  # Default to WGS84\n",
        "    \n",
        "    saved_files = []\n",
        "    \n",
        "    # 1. Save per-year productivity index GeoTIFFs\n",
        "    for i, index_array in enumerate(per_year_productivity_indices):\n",
        "        output_filename = os.path.join(output_dir, f'productivity_index_year_{i+1:02d}.tif')\n",
        "        try:\n",
        "            with rasterio.open(\n",
        "                output_filename,\n",
        "                'w',\n",
        "                driver='GTiff',\n",
        "                height=index_array.shape[0],\n",
        "                width=index_array.shape[1],\n",
        "                count=1,\n",
        "                dtype='float32',\n",
        "                crs=save_crs,\n",
        "                transform=save_transform,\n",
        "                compress='lzw',\n",
        "                nodata=np.nan\n",
        "            ) as dst:\n",
        "                dst.write(index_array.astype(np.float32), 1)\n",
        "                # Add metadata\n",
        "                dst.update_tags(\n",
        "                    description=f\"Productivity Index Year {i+1}\",\n",
        "                    units=\"ratio\",\n",
        "                    interpretation=\"1.0 = field average, >1.0 = above average, <1.0 = below average\"\n",
        "                )\n",
        "            saved_files.append(output_filename)\n",
        "            print(f\"  \u2713 Year {i+1} productivity index: {output_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  \u2717 Error saving year {i+1} index: {e}\")\n",
        "    \n",
        "    # 2. Save composite productivity index GeoTIFF\n",
        "    composite_filename = os.path.join(output_dir, 'composite_productivity_index.tif')\n",
        "    try:\n",
        "        with rasterio.open(\n",
        "            composite_filename,\n",
        "            'w',\n",
        "            driver='GTiff',\n",
        "            height=composite_productivity_index.shape[0],\n",
        "            width=composite_productivity_index.shape[1],\n",
        "            count=1,\n",
        "            dtype='float32',\n",
        "            crs=save_crs,\n",
        "            transform=save_transform,\n",
        "            compress='lzw',\n",
        "            nodata=np.nan\n",
        "        ) as dst:\n",
        "            dst.write(composite_productivity_index.astype(np.float32), 1)\n",
        "            dst.update_tags(\n",
        "                description=\"Multi-year Composite Productivity Index\",\n",
        "                units=\"ratio\",\n",
        "                interpretation=\"1.0 = field average, >1.0 = above average, <1.0 = below average\",\n",
        "                years_averaged=str(len(per_year_productivity_indices))\n",
        "            )\n",
        "        saved_files.append(composite_filename)\n",
        "        print(f\"  \u2713 Composite productivity index: {composite_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Error saving composite index: {e}\")\n",
        "    \n",
        "    # 3. Save 3-zone classification GeoTIFF with color table\n",
        "    zones_filename = os.path.join(output_dir, 'productivity_zones.tif')\n",
        "    try:\n",
        "        with rasterio.open(\n",
        "            zones_filename,\n",
        "            'w',\n",
        "            driver='GTiff',\n",
        "            height=three_class_map.shape[0],\n",
        "            width=three_class_map.shape[1],\n",
        "            count=1,\n",
        "            dtype='int16',\n",
        "            crs=save_crs,\n",
        "            transform=save_transform,\n",
        "            compress='lzw',\n",
        "            nodata=-9999\n",
        "        ) as dst:\n",
        "            # Convert NaN to nodata value for integer raster\n",
        "            zones_int = three_class_map.copy()\n",
        "            zones_int[np.isnan(zones_int)] = -9999\n",
        "            dst.write(zones_int.astype(np.int16), 1)\n",
        "            \n",
        "            # Add color interpretation (red-yellow-green for low-medium-high)\n",
        "            dst.write_colormap(1, {\n",
        "                1: (255, 67, 67, 255),    # Red for low productivity\n",
        "                2: (255, 255, 102, 255),  # Yellow for medium\n",
        "                3: (76, 230, 0, 255),     # Green for high\n",
        "                -9999: (0, 0, 0, 0)       # Transparent for nodata\n",
        "            })\n",
        "            \n",
        "            # Add metadata\n",
        "            if 'zone_thresholds' in locals() and zone_thresholds:\n",
        "                dst.update_tags(\n",
        "                    description=\"Productivity Zones (3-class)\",\n",
        "                    classification_method=zone_thresholds.get('method', 'quantile_tertiles'),\n",
        "                    zone_1=\"Low productivity\",\n",
        "                    zone_2=\"Medium productivity\", \n",
        "                    zone_3=\"High productivity\",\n",
        "                    lower_threshold=str(zone_thresholds.get('lower', '')),\n",
        "                    upper_threshold=str(zone_thresholds.get('upper', ''))\n",
        "                )\n",
        "        saved_files.append(zones_filename)\n",
        "        print(f\"  \u2713 Productivity zones: {zones_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Error saving zones: {e}\")\n",
        "    \n",
        "    # 4. Create summary statistics CSV\n",
        "    summary_filename = os.path.join(output_dir, 'productivity_summary.csv')\n",
        "    try:\n",
        "        summary_data = []\n",
        "        \n",
        "        # Per-year statistics\n",
        "        for i, (mean_ndvi, index_array) in enumerate(zip(mean_ndvi_values_recalc, per_year_productivity_indices)):\n",
        "            valid_index = index_array[~np.isnan(index_array)]\n",
        "            if len(valid_index) > 0:\n",
        "                summary_data.append({\n",
        "                    'Year': i + 1,\n",
        "                    'Mean_NDVI': mean_ndvi if not np.isnan(mean_ndvi) else None,\n",
        "                    'Index_Mean': np.mean(valid_index),\n",
        "                    'Index_StdDev': np.std(valid_index),\n",
        "                    'Index_Min': np.min(valid_index),\n",
        "                    'Index_Max': np.max(valid_index),\n",
        "                    'Valid_Pixels': len(valid_index)\n",
        "                })\n",
        "        \n",
        "        # Composite statistics\n",
        "        valid_composite = composite_productivity_index[~np.isnan(composite_productivity_index)]\n",
        "        if len(valid_composite) > 0:\n",
        "            summary_data.append({\n",
        "                'Year': 'Composite',\n",
        "                'Mean_NDVI': None,\n",
        "                'Index_Mean': np.mean(valid_composite),\n",
        "                'Index_StdDev': np.std(valid_composite),\n",
        "                'Index_Min': np.min(valid_composite),\n",
        "                'Index_Max': np.max(valid_composite),\n",
        "                'Valid_Pixels': len(valid_composite)\n",
        "            })\n",
        "        \n",
        "        # Zone thresholds\n",
        "        if 'zone_thresholds' in locals() and zone_thresholds:\n",
        "            summary_data.append({\n",
        "                'Year': 'Zone_Thresholds',\n",
        "                'Mean_NDVI': None,\n",
        "                'Index_Mean': zone_thresholds.get('lower'),\n",
        "                'Index_StdDev': zone_thresholds.get('upper'),\n",
        "                'Index_Min': None,\n",
        "                'Index_Max': None,\n",
        "                'Valid_Pixels': None\n",
        "            })\n",
        "        \n",
        "        # Save to CSV\n",
        "        df_summary = pd.DataFrame(summary_data)\n",
        "        df_summary.to_csv(summary_filename, index=False)\n",
        "        saved_files.append(summary_filename)\n",
        "        print(f\"  \u2713 Summary statistics: {summary_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  \u2717 Error saving summary CSV: {e}\")\n",
        "    \n",
        "    # 5. Create ZIP archive for download\n",
        "    zip_filename = 'productivity_outputs.zip'\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "            for file in saved_files:\n",
        "                if os.path.exists(file):\n",
        "                    zipf.write(file, os.path.basename(file))\n",
        "        print(f\"\\n\u2713 Created ZIP archive: {zip_filename}\")\n",
        "        \n",
        "        # Provide download mechanism based on environment\n",
        "        if IN_COLAB:\n",
        "            from google.colab import files\n",
        "            files.download(zip_filename)\n",
        "            print(\"Download initiated via Colab...\")\n",
        "        else:\n",
        "            print(f\"Download the outputs from: {os.path.abspath(zip_filename)}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"\u2717 Error creating ZIP archive: {e}\")\n",
        "    \n",
        "    print(f\"\\nProcessing complete! {len(saved_files)} files saved.\")\n",
        "    print(\"Files include:\")\n",
        "    print(f\"  - {len(per_year_productivity_indices)} per-year productivity index rasters\")\n",
        "    print(f\"  - 1 composite productivity index raster\")\n",
        "    print(f\"  - 1 productivity zones raster (3 classes)\")\n",
        "    print(f\"  - 1 summary statistics CSV\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directory: productivity_outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6b859a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of required data and then proceed to save the results to GeoTIFF and CSV files in the output directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFC1mdIv9E4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58e4d402",
        "outputId": "4612004a-8175-47cd-ce64-b5a83e99d16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving output files to 'productivity_outputs'...\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_1.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_2.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_3.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_4.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_5.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_6.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_7.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_8.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_9.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_10.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_11.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_12.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_13.tif\n",
            "Saved per-year productivity GeoTIFF: productivity_outputs/per_year_productivity_14.tif\n",
            "Saved Composite Productivity GeoTIFF: productivity_outputs/composite_productivity.tif\n",
            "Saved 3-class map GeoTIFF: productivity_outputs/three_class_map.tif\n",
            "Saved CSV summary: productivity_outputs/productivity_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check if required data is available\n",
        "if not per_year_productivity_indices or composite_productivity_index is None or three_class_map is None or not mean_ndvi_values or not ndvi_datasets:\n",
        "    print(\"Required data (per-year productivity indices, composite index, 3-class map, mean NDVI, or original NDVI datasets) is not fully available. Skipping saving.\")\n",
        "else:\n",
        "    print(f\"Saving output files to '{output_dir}'...\")\n",
        "\n",
        "    # Assuming the structure of ndvi_datasets is now (clipped_array, transform) or just numpy arrays\n",
        "    # We need the original transforms and CRSs for saving.\n",
        "    # If clipping was successful, the transform is within the tuple.\n",
        "    # If clipping was skipped, ndvi_datasets still holds the aligned numpy arrays.\n",
        "    # We need to access the original rasterio datasets to get the CRS and original transform\n",
        "    # before clipping and alignment.\n",
        "    # Assuming original_ndvi_datasets (list of rasterio dataset objects) is available\n",
        "    # from the loading step. This was not explicitly preserved.\n",
        "    # Let's assume we stored the target_transform and target_crs from the alignment step.\n",
        "\n",
        "    try:\n",
        "        target_transform\n",
        "        target_crs\n",
        "    except NameError:\n",
        "        print(\"Target transform or CRS not defined from previous steps. Cannot save GeoTIFFs.\")\n",
        "    else:\n",
        "        # Save per-year Productivity GeoTIFFs\n",
        "        for i, per_year_index_array in enumerate(per_year_productivity_indices):\n",
        "            # Use the target transform and CRS for saving\n",
        "            output_filename = os.path.join(output_dir, f'per_year_productivity_{i+1}.tif')\n",
        "            try:\n",
        "                height, width = per_year_index_array.shape\n",
        "                dtype = per_year_index_array.dtype\n",
        "                with rasterio.open(\n",
        "                    output_filename,\n",
        "                    'w',\n",
        "                    driver='GTiff',\n",
        "                    height=height,\n",
        "                    width=width,\n",
        "                    count=1,\n",
        "                    dtype=dtype,\n",
        "                    crs=target_crs,\n",
        "                    transform=target_transform,\n",
        "                    nodata=np.nan # Specify NoData value\n",
        "                ) as dst:\n",
        "                    dst.write(per_year_index_array, 1)\n",
        "                print(f\"Saved per-year productivity GeoTIFF: {output_filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving per-year productivity GeoTIFF {output_filename}: {e}\")\n",
        "\n",
        "        # Save Composite GeoTIFF\n",
        "        output_composite_filename = os.path.join(output_dir, 'composite_productivity.tif')\n",
        "        try:\n",
        "            height, width = composite_productivity_index.shape\n",
        "            dtype = composite_productivity_index.dtype\n",
        "            with rasterio.open(\n",
        "                output_composite_filename,\n",
        "                'w',\n",
        "                driver='GTiff',\n",
        "                height=height,\n",
        "                width=width,\n",
        "                count=1,\n",
        "                dtype=dtype,\n",
        "                crs=target_crs,\n",
        "                transform=target_transform,\n",
        "                nodata=np.nan # Specify NoData value\n",
        "            ) as dst:\n",
        "                dst.write(composite_productivity_index, 1)\n",
        "            print(f\"Saved Composite Productivity GeoTIFF: {output_composite_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving Composite Productivity GeoTIFF {output_composite_filename}: {e}\")\n",
        "\n",
        "        # Save 3-class map GeoTIFF\n",
        "        output_map_filename = os.path.join(output_dir, 'three_class_map.tif')\n",
        "        try:\n",
        "            height, width = three_class_map.shape\n",
        "            # The data type of the 3-class map is likely integer or float,\n",
        "            # let's ensure it's an appropriate type for GeoTIFF.\n",
        "            # Using float32 for consistency, but integer type might be better\n",
        "            # depending on the required output.\n",
        "            dtype = np.float32\n",
        "            with rasterio.open(\n",
        "                output_map_filename,\n",
        "                'w',\n",
        "                driver='GTiff',\n",
        "                height=height,\n",
        "                width=width,\n",
        "                count=1,\n",
        "                dtype=dtype,\n",
        "                crs=target_crs,\n",
        "                transform=target_transform,\n",
        "                nodata=np.nan # Use NaN or a specific integer for NoData if using integer dtype\n",
        "            ) as dst:\n",
        "                dst.write(three_class_map.astype(dtype), 1) # Ensure correct dtype is written\n",
        "            print(f\"Saved 3-class map GeoTIFF: {output_map_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving 3-class map GeoTIFF {output_map_filename}: {e}\")\n",
        "\n",
        "    # Create and save CSV summary\n",
        "    # Assuming we have filenames available from the upload step (ndvi_uploads.value.keys())\n",
        "    # and corresponding mean_ndvi_values and potentially mean productivity indices.\n",
        "    # We don't have mean productivity indices calculated explicitly.\n",
        "    # Let's create a DataFrame with filenames (if available) and mean_ndvi_values.\n",
        "\n",
        "    summary_data = {'Mean_NDVI': mean_ndvi_values}\n",
        "\n",
        "    # Try to get original filenames if ndvi_uploads is available and populated\n",
        "    if 'ndvi_uploads' in locals() and ndvi_uploads.value:\n",
        "         # Get filenames in the order they were likely processed\n",
        "         # Assuming the order in ndvi_uploads.value.keys() matches the order in mean_ndvi_values\n",
        "         filenames = list(ndvi_uploads.value.keys())[:len(mean_ndvi_values)]\n",
        "         summary_data['Original_Filename'] = filenames\n",
        "         # Reorder columns\n",
        "         summary_df = pd.DataFrame(summary_data, columns=['Original_Filename', 'Mean_NDVI'])\n",
        "    else:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "\n",
        "    output_csv_filename = os.path.join(output_dir, 'productivity_summary.csv')\n",
        "    try:\n",
        "        summary_df.to_csv(output_csv_filename, index=False)\n",
        "        print(f\"Saved CSV summary: {output_csv_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving CSV summary {output_csv_filename}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cd559eb"
      },
      "source": [
        "## Download Outputs\n",
        "\n",
        "### Subtask:\n",
        "Zip the output folder and provide a download link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "c72da0bd",
        "outputId": "fde9e194-40d0-4aa6-b5d0-230c46835376"
      },
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Define the output directory\n",
        "output_dir = 'productivity_outputs'\n",
        "zip_filename = f'{output_dir}.zip'\n",
        "\n",
        "# Create a zip archive of the output directory\n",
        "try:\n",
        "    shutil.make_archive(output_dir, 'zip', output_dir)\n",
        "    print(f\"Created zip archive: {zip_filename}\")\n",
        "\n",
        "    # Provide a download link in Colab\n",
        "    if IN_COLAB:\n",
        "        print(\"Click the link below to download the zip file:\")\n",
        "        files.download(zip_filename)\n",
        "    else:\n",
        "        print(f\"Zip file created at: {zip_filename}. You can download it manually.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Output directory '{output_dir}' not found. Please ensure the saving step was successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error zipping and downloading files: {e}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created zip archive: productivity_outputs.zip\n",
            "Click the link below to download the zip file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3859f114-0b67-412c-9579-f66485b1c1e5\", \"productivity_outputs.zip\", 364116)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e17c8f"
      },
      "source": [
        "# Task\n",
        "Analyze the relationship between the calculated productivity indices and the provided DSM and TPI raster data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d02332a"
      },
      "source": [
        "## Upload dsm and tpi data\n",
        "\n",
        "### Subtask:\n",
        "Add cells to upload the DSM and TPI raster files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38c34398"
      },
      "source": [
        "**Reasoning**:\n",
        "Create and display the file upload widgets for DSM and TPI rasters as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "55492f3e474340baa766b1a502353739",
            "b426b8273b1a4688b3254ae185fc52aa",
            "4c9d5a89a230491fa81d4698352b335b",
            "f33efd3b813a4172864c2bef7110ce7e",
            "cb2d16782dd74eee839b03704f1ae6e1",
            "5bd413855a324664941c98cd7ed5bd8d"
          ]
        },
        "id": "2e119132",
        "outputId": "8ebf3c12-7b8f-4c2d-b622-7820e487d83a"
      },
      "source": [
        "# Create file upload widgets for DSM and TPI\n",
        "dsm_uploads = widgets.FileUpload(\n",
        "    accept='.tif,.tiff',\n",
        "    multiple=True,\n",
        "    description='Upload DSM Rasters (GeoTIFFs)'\n",
        ")\n",
        "\n",
        "tpi_uploads = widgets.FileUpload(\n",
        "    accept='.tif,.tiff',\n",
        "    multiple=True,\n",
        "    description='Upload TPI Rasters (GeoTIFFs)'\n",
        ")\n",
        "\n",
        "display(dsm_uploads)\n",
        "display(tpi_uploads)\n",
        "print(\"File upload widgets created for DSM and TPI. Please upload the respective files.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.tif,.tiff', description='Upload DSM Rasters (GeoTIFFs)', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55492f3e474340baa766b1a502353739"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.tif,.tiff', description='Upload TPI Rasters (GeoTIFFs)', multiple=True)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f33efd3b813a4172864c2bef7110ce7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File upload widgets created for DSM and TPI. Please upload the respective files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae543098"
      },
      "source": [
        "## Load dsm and tpi data\n",
        "\n",
        "### Subtask:\n",
        "Load the uploaded DSM and TPI rasters into rasterio datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d383ecf7"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for uploaded DSM and TPI files and load them into rasterio datasets using MemoryFile, then store the datasets and initialize lists for later use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4db30a66",
        "outputId": "fb0450b5-6d56-4250-934b-44db6b6b3f40"
      },
      "source": [
        "original_dsm_datasets = []\n",
        "original_tpi_datasets = []\n",
        "dsm_memfiles = [] # Keep MemoryFile objects alive for DSM\n",
        "tpi_memfiles = [] # Keep MemoryFile objects alive for TPI\n",
        "\n",
        "# Load DSM rasters\n",
        "dsm_files = get_file_content(dsm_uploads, is_multiple=True)\n",
        "if dsm_files:\n",
        "    for file_data in dsm_files:\n",
        "        try:\n",
        "            memfile = rasterio.io.MemoryFile(file_data['content'])\n",
        "            dataset = memfile.open()\n",
        "            original_dsm_datasets.append(dataset)\n",
        "            dsm_memfiles.append(memfile) # Important: Keep memfile alive!\n",
        "            print(f\"DSM raster '{file_data['name']}' loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading DSM file '{file_data['name']}': {e}\")\n",
        "else:\n",
        "    print(\"No DSM files uploaded. Skipping DSM loading.\")\n",
        "\n",
        "# Load TPI rasters\n",
        "tpi_files = get_file_content(tpi_uploads, is_multiple=True)\n",
        "if tpi_files:\n",
        "    for file_data in tpi_files:\n",
        "        try:\n",
        "            memfile = rasterio.io.MemoryFile(file_data['content'])\n",
        "            dataset = memfile.open()\n",
        "            original_tpi_datasets.append(dataset)\n",
        "            tpi_memfiles.append(memfile) # Important: Keep memfile alive!\n",
        "            print(f\"TPI raster '{file_data['name']}' loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading TPI file '{file_data['name']}': {e}\")\n",
        "else:\n",
        "    print(\"No TPI files uploaded. Skipping TPI loading.\")\n",
        "\n",
        "# Initialize dsm_datasets and tpi_datasets for later use\n",
        "dsm_datasets = []\n",
        "tpi_datasets = []\n",
        "\n",
        "# Summary\n",
        "print(f\"\\nSummary: Loaded {len(original_dsm_datasets)} DSM raster(s) and {len(original_tpi_datasets)} TPI raster(s).\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No DSM files uploaded. Skipping DSM loading.\n",
            "No TPI files uploaded. Skipping TPI loading.\n",
            "\n",
            "Summary: Loaded 0 DSM raster(s) and 0 TPI raster(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6890094"
      },
      "source": [
        "## Reproject and align dsm and tpi\n",
        "\n",
        "### Subtask:\n",
        "Reproject and align the DSM and TPI rasters to match the grid and CRS of the NDVI data (or a common reference).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266f81fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of DSM and TPI datasets and the target reference variables, then reproject and align the available datasets to the target grid and CRS, storing the results in new lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "809f6c0c",
        "outputId": "6ba9a6f2-d504-4281-bf6d-0068733fe763"
      },
      "source": [
        "aligned_dsm_arrays = []\n",
        "aligned_tpi_arrays = []\n",
        "\n",
        "# 1. Check if there are any original_dsm_datasets or original_tpi_datasets to process.\n",
        "if not original_dsm_datasets and not original_tpi_datasets:\n",
        "    print(\"No original DSM or TPI datasets loaded. Skipping reprojection and alignment for DSM/TPI.\")\n",
        "else:\n",
        "    # 2. Determine the target coordinate reference system (CRS), transform, width, and height.\n",
        "    try:\n",
        "        target_crs\n",
        "        target_transform\n",
        "        target_width\n",
        "        target_height\n",
        "        print(f\"Using target CRS: {target_crs}, transform, width: {target_width}, height: {target_height} from NDVI alignment.\")\n",
        "    except NameError:\n",
        "        print(\"Target CRS, transform, width, or height from NDVI alignment are not available. Cannot reproject/align DSM/TPI.\")\n",
        "    else:\n",
        "        # 4. Iterate through the original_dsm_datasets (if any) and reproject/align them.\n",
        "        if original_dsm_datasets:\n",
        "            print(\"Processing DSM datasets for reprojection and alignment...\")\n",
        "            for i, dataset in enumerate(original_dsm_datasets):\n",
        "                try:\n",
        "                    reprojected_data = np.empty((target_height, target_width), dtype=dataset.dtypes[0])\n",
        "\n",
        "                    rasterio.warp.reproject(\n",
        "                        source=rasterio.band(dataset, 1),\n",
        "                        destination=reprojected_data,\n",
        "                        src_transform=dataset.transform,\n",
        "                        src_crs=dataset.crs,\n",
        "                        dst_transform=target_transform,\n",
        "                        dst_crs=target_crs,\n",
        "                        resampling=rasterio.warp.Resampling.bilinear,\n",
        "                        dst_nodata=dataset.nodata # Use source nodata for destination\n",
        "                    )\n",
        "                    aligned_dsm_arrays.append(reprojected_data)\n",
        "                    print(f\"DSM dataset {i} reprojected and aligned.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing DSM dataset {i}: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "        # 5. Iterate through the original_tpi_datasets (if any) and reproject/align them.\n",
        "        if original_tpi_datasets:\n",
        "            print(\"Processing TPI datasets for reprojection and alignment...\")\n",
        "            for i, dataset in enumerate(original_tpi_datasets):\n",
        "                try:\n",
        "                    reprojected_data = np.empty((target_height, target_width), dtype=dataset.dtypes[0])\n",
        "\n",
        "                    rasterio.warp.reproject(\n",
        "                        source=rasterio.band(dataset, 1),\n",
        "                        destination=reprojected_data,\n",
        "                        src_transform=dataset.transform,\n",
        "                        src_crs=dataset.crs,\n",
        "                        dst_transform=target_transform,\n",
        "                        dst_crs=target_crs,\n",
        "                        resampling=rasterio.warp.Resampling.bilinear,\n",
        "                        dst_nodata=dataset.nodata # Use source nodata for destination\n",
        "                    )\n",
        "                    aligned_tpi_arrays.append(reprojected_data)\n",
        "                    print(f\"TPI dataset {i} reprojected and aligned.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing TPI dataset {i}: {e}\")\n",
        "                    import traceback\n",
        "                    traceback.print_exc()\n",
        "\n",
        "        # 6. After processing, update dsm_datasets with aligned_dsm_arrays and tpi_datasets with aligned_tpi_arrays.\n",
        "        dsm_datasets = aligned_dsm_arrays\n",
        "        tpi_datasets = aligned_tpi_arrays\n",
        "\n",
        "        # 7. Print a summary message\n",
        "        print(f\"\\nReprojection and alignment complete: {len(dsm_datasets)} DSM and {len(tpi_datasets)} TPI datasets are ready.\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No original DSM or TPI datasets loaded. Skipping reprojection and alignment for DSM/TPI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc64220e"
      },
      "source": [
        "## Clip dsm and tpi to aoi\n",
        "\n",
        "### Subtask:\n",
        "Clip the aligned DSM and TPI rasters to the AOI polygon.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95dbbc7"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of AOI, DSM, and TPI data, then iterate through DSM and TPI datasets (if available), create temporary in-memory datasets with the target transform and CRS, clip them using the AOI geometry, store the clipped data and transforms, update the dsm_datasets and tpi_datasets lists, and print a summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdb5219f",
        "outputId": "a14c3c7e-61c0-4c4c-d944-1c38e2d54eeb"
      },
      "source": [
        "import rasterio.mask\n",
        "\n",
        "clipped_dsm_data_and_transforms = []\n",
        "clipped_tpi_data_and_transforms = []\n",
        "\n",
        "# 1. Check if both aoi_gdf and either dsm_datasets or tpi_datasets are available.\n",
        "if aoi_gdf is None or (not dsm_datasets and not tpi_datasets):\n",
        "    print(\"AOI polygon, DSM datasets, or TPI datasets not available. Skipping clipping process for DSM/TPI.\")\n",
        "else:\n",
        "    # 2. Ensure the AOI is in the same CRS as the target CRS used for alignment (target_crs).\n",
        "    try:\n",
        "        target_crs\n",
        "        target_transform\n",
        "    except NameError:\n",
        "        print(\"Target CRS or Transform not defined from previous steps. Cannot clip DSM/TPI.\")\n",
        "    else:\n",
        "        if aoi_gdf.crs != target_crs:\n",
        "            try:\n",
        "                aoi_gdf = aoi_gdf.to_crs(target_crs)\n",
        "                print(f\"AOI polygon reprojected to target CRS: {target_crs}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reprojecting AOI polygon: {e}\")\n",
        "                aoi_gdf = None # Set to None if reprojection fails\n",
        "\n",
        "        if aoi_gdf is not None:\n",
        "            # 3. Get the geometry of the aoi_gdf for masking.\n",
        "            geometries = aoi_gdf.geometry.values\n",
        "\n",
        "            # 4. If dsm_datasets are available, iterate and clip them.\n",
        "            if dsm_datasets:\n",
        "                print(\"Clipping DSM datasets to AOI...\")\n",
        "                for i, dsm_array in enumerate(dsm_datasets):\n",
        "                    try:\n",
        "                        # Create a temporary in-memory dataset for clipping\n",
        "                        height, width = dsm_array.shape\n",
        "                        dtype = dsm_array.dtype\n",
        "\n",
        "                        with rasterio.MemoryFile() as memfile:\n",
        "                            with memfile.open(\n",
        "                                driver='GTiff',\n",
        "                                height=height,\n",
        "                                width=width,\n",
        "                                count=1,\n",
        "                                dtype=dtype,\n",
        "                                crs=target_crs,\n",
        "                                transform=target_transform\n",
        "                            ) as tmp_dataset:\n",
        "                                tmp_dataset.write(dsm_array, 1)\n",
        "\n",
        "                                # Perform clipping\n",
        "                                clipped_data, clipped_transform = rasterio.mask.mask(\n",
        "                                    tmp_dataset, geometries, crop=True, nodata=np.nan # Use np.nan for NoData\n",
        "                                )\n",
        "                                clipped_data = clipped_data[0]  # Get first band\n",
        "\n",
        "                        clipped_dsm_data_and_transforms.append((clipped_data, clipped_transform))\n",
        "                        print(f\"DSM dataset {i} clipped successfully.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error clipping DSM dataset {i}: {e}\")\n",
        "                        import traceback\n",
        "                        traceback.print_exc()\n",
        "\n",
        "            # 5. If tpi_datasets are available, iterate and clip them.\n",
        "            if tpi_datasets:\n",
        "                print(\"Clipping TPI datasets to AOI...\")\n",
        "                for i, tpi_array in enumerate(tpi_datasets):\n",
        "                    try:\n",
        "                        # Create a temporary in-memory dataset for clipping\n",
        "                        height, width = tpi_array.shape\n",
        "                        dtype = tpi_array.dtype\n",
        "\n",
        "                        with rasterio.MemoryFile() as memfile:\n",
        "                            with memfile.open(\n",
        "                                driver='GTiff',\n",
        "                                height=height,\n",
        "                                width=width,\n",
        "                                count=1,\n",
        "                                dtype=dtype,\n",
        "                                crs=target_crs,\n",
        "                                transform=target_transform\n",
        "                            ) as tmp_dataset:\n",
        "                                tmp_dataset.write(tpi_array, 1)\n",
        "\n",
        "                                # Perform clipping\n",
        "                                clipped_data, clipped_transform = rasterio.mask.mask(\n",
        "                                    tmp_dataset, geometries, crop=True, nodata=np.nan # Use np.nan for NoData\n",
        "                                )\n",
        "                                clipped_data = clipped_data[0]  # Get first band\n",
        "\n",
        "                        clipped_tpi_data_and_transforms.append((clipped_data, clipped_transform))\n",
        "                        print(f\"TPI dataset {i} clipped successfully.\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error clipping TPI dataset {i}: {e}\")\n",
        "                        import traceback\n",
        "                        traceback.print_exc()\n",
        "\n",
        "        # 6. After processing, update dsm_datasets and tpi_datasets.\n",
        "        dsm_datasets = clipped_dsm_data_and_transforms\n",
        "        tpi_datasets = clipped_tpi_data_and_transforms\n",
        "\n",
        "        # 7. Print a summary message\n",
        "        print(f\"\\nClipping complete: {len(dsm_datasets)} DSM and {len(tpi_datasets)} TPI datasets clipped to AOI.\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AOI polygon, DSM datasets, or TPI datasets not available. Skipping clipping process for DSM/TPI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a075b2f5"
      },
      "source": [
        "## Analyze relationship between productivity and topography\n",
        "\n",
        "### Subtask:\n",
        "Explore the relationship between the calculated productivity indices (per-year or composite) and the DSM and TPI values within the AOI. This could involve:\n",
        "    - Zonal statistics: Calculate mean or median DSM/TPI within each productivity class.\n",
        "    - Scatter plots: Plot productivity values against DSM or TPI values for individual pixels.\n",
        "    - Spatial analysis: Identify areas of high/low productivity that correlate with specific topographic features (e.g., ridges, valleys).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad30cda0"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of required data and if available, extract, flatten, and create a DataFrame from the composite productivity index, clipped DSM, and clipped TPI data. Then, calculate and print the correlation matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93da3788",
        "outputId": "89cfc11b-a595-4729-ba8f-c160ac7ad4d0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Check if required data is available\n",
        "if composite_productivity_index is None or not clipped_dsm_data_and_transforms or not clipped_tpi_data_and_transforms:\n",
        "    print(\"Required data (composite productivity index, clipped DSM, or clipped TPI) is not available. Skipping relationship analysis.\")\n",
        "else:\n",
        "    print(\"Analyzing relationship between productivity index, DSM, and TPI...\")\n",
        "\n",
        "    # 2. Extract the clipped DSM and TPI arrays\n",
        "    # Assuming there is only one DSM and one TPI raster\n",
        "    clipped_dsm_array, _ = clipped_dsm_data_and_transforms[0]\n",
        "    clipped_tpi_array, _ = clipped_tpi_data_and_transforms[0]\n",
        "\n",
        "    # Ensure the arrays have the same shape as the composite index\n",
        "    if composite_productivity_index.shape != clipped_dsm_array.shape or composite_productivity_index.shape != clipped_tpi_array.shape:\n",
        "        print(\"Error: Composite productivity index, DSM, and TPI arrays do not have consistent shapes. Cannot proceed with analysis.\")\n",
        "    else:\n",
        "        # 3. Flatten the arrays, ignoring NaN values\n",
        "        composite_flat = composite_productivity_index[~np.isnan(composite_productivity_index)].flatten()\n",
        "        dsm_flat = clipped_dsm_array[~np.isnan(composite_productivity_index)].flatten() # Use composite index mask to align\n",
        "        tpi_flat = clipped_tpi_array[~np.isnan(composite_productivity_index)].flatten() # Use composite index mask to align\n",
        "\n",
        "        # Remove NaN values from the flattened arrays to ensure they have the same length\n",
        "        # Create a combined mask for NaN values across all arrays\n",
        "        combined_nan_mask = np.isnan(composite_flat) | np.isnan(dsm_flat) | np.isnan(tpi_flat)\n",
        "\n",
        "        composite_flat_cleaned = composite_flat[~combined_nan_mask]\n",
        "        dsm_flat_cleaned = dsm_flat[~combined_nan_mask]\n",
        "        tpi_flat_cleaned = tpi_flat[~combined_nan_mask]\n",
        "\n",
        "\n",
        "        if composite_flat_cleaned.size == 0:\n",
        "             print(\"No valid data points available after removing NaN values. Skipping analysis.\")\n",
        "        else:\n",
        "            # 4. Create a pandas DataFrame\n",
        "            analysis_df = pd.DataFrame({\n",
        "                'Composite_Productivity': composite_flat_cleaned,\n",
        "                'DSM': dsm_flat_cleaned,\n",
        "                'TPI': tpi_flat_cleaned\n",
        "            })\n",
        "\n",
        "            # 5. Calculate and print the correlation matrix\n",
        "            correlation_matrix = analysis_df.corr()\n",
        "            print(\"\\nCorrelation Matrix:\")\n",
        "            display(correlation_matrix)\n",
        "\n",
        "            # 6. Generate scatter plots\n",
        "            print(\"\\nGenerating Scatter Plots:\")\n",
        "\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.scatter(analysis_df['DSM'], analysis_df['Composite_Productivity'], alpha=0.5, s=10)\n",
        "            plt.title('Composite Productivity vs. DSM')\n",
        "            plt.xlabel('DSM')\n",
        "            plt.ylabel('Composite Productivity Index')\n",
        "            plt.grid(True)\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.scatter(analysis_df['TPI'], analysis_df['Composite_Productivity'], alpha=0.5, s=10, color='orange')\n",
        "            plt.title('Composite Productivity vs. TPI')\n",
        "            plt.xlabel('TPI')\n",
        "            plt.ylabel('Composite Productivity Index')\n",
        "            plt.grid(True)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 7. Print a summary of the correlation analysis\n",
        "            print(\"\\nCorrelation Analysis Summary:\")\n",
        "            print(\"The correlation matrix shows the linear relationship between the variables.\")\n",
        "            print(f\"- Correlation between Composite Productivity and DSM: {correlation_matrix.loc['Composite_Productivity', 'DSM']:.2f}\")\n",
        "            print(f\"- Correlation between Composite Productivity and TPI: {correlation_matrix.loc['Composite_Productivity', 'TPI']:.2f}\")\n",
        "            print(\"Scatter plots visualize these relationships.\")\n",
        "            print(\"A correlation value close to 1 indicates a strong positive linear relationship, close to -1 indicates a strong negative linear relationship, and close to 0 indicates a weak linear relationship.\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required data (composite productivity index, clipped DSM, or clipped TPI) is not available. Skipping relationship analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7bed688"
      },
      "source": [
        "## Analyze relationship between productivity and topography - retry 1\n",
        "\n",
        "### Subtask:\n",
        "Retry: Explore the relationship between the calculated productivity indices (per-year or composite) and the DSM and TPI values within the AOI. This could involve:\n",
        "    - Zonal statistics: Calculate mean or median DSM/TPI within each productivity class.\n",
        "    - Scatter plots: Plot productivity values against DSM or TPI values for individual pixels.\n",
        "    - Spatial analysis: Identify areas of high/low productivity that correlate with specific topographic features (e.g., ridges, valleys).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90faa848"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of required data and then proceed to perform the relationship analysis, including creating a DataFrame, calculating the correlation matrix, generating scatter plots, performing zonal statistics based on the 3-class map, and finally summarizing the findings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c46562a5",
        "outputId": "6f128eee-e854-4b6c-bbe5-590735baf281"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio.features\n",
        "\n",
        "# 1. Check if required data is available\n",
        "# Ensure all necessary variables are available before proceeding\n",
        "required_vars = ['composite_productivity_index', 'three_class_map', 'clipped_dsm_data_and_transforms', 'clipped_tpi_data_and_transforms', 'target_transform']\n",
        "if not all(var in locals() and locals()[var] is not None for var in ['composite_productivity_index', 'three_class_map']) or \\\n",
        "   not clipped_dsm_data_and_transforms or not clipped_tpi_data_and_transforms or \\\n",
        "   'target_transform' not in locals():\n",
        "    print(\"Required data (composite productivity index, 3-class map, clipped DSM, clipped TPI, or target transform) is not fully available. Skipping relationship analysis.\")\n",
        "else:\n",
        "    print(\"Analyzing relationship between productivity index, DSM, and TPI...\")\n",
        "\n",
        "    # 2. Extract the first clipped DSM and TPI arrays and their transforms\n",
        "    # Assuming there is only one DSM and one TPI raster\n",
        "    clipped_dsm_array, clipped_dsm_transform = clipped_dsm_data_and_transforms[0]\n",
        "    clipped_tpi_array, clipped_tpi_transform = clipped_tpi_data_and_transforms[0]\n",
        "\n",
        "    # Ensure the arrays have consistent shapes\n",
        "    if composite_productivity_index.shape != clipped_dsm_array.shape or composite_productivity_index.shape != clipped_tpi_array.shape or composite_productivity_index.shape != three_class_map.shape:\n",
        "        print(f\"Error: Array shapes are inconsistent:\")\n",
        "        print(f\"  Composite Productivity: {composite_productivity_index.shape}\")\n",
        "        print(f\"  Clipped DSM: {clipped_dsm_array.shape}\")\n",
        "        print(f\"  Clipped TPI: {clipped_tpi_array.shape}\")\n",
        "        print(f\"  3-Class Map: {three_class_map.shape}\")\n",
        "        print(\"Cannot proceed with analysis.\")\n",
        "    else:\n",
        "        # 3. Flatten the arrays, handling NaN values consistently\n",
        "        # Create a mask for valid data points (where composite_productivity_index is not NaN)\n",
        "        valid_mask = ~np.isnan(composite_productivity_index)\n",
        "\n",
        "        composite_flat_masked = composite_productivity_index[valid_mask].flatten()\n",
        "        dsm_flat_masked = clipped_dsm_array[valid_mask].flatten()\n",
        "        tpi_flat_masked = clipped_tpi_array[valid_mask].flatten()\n",
        "        three_class_flat_masked = three_class_map[valid_mask].flatten()\n",
        "\n",
        "\n",
        "        # Further remove NaNs that might be present in DSM or TPI within the valid productivity areas\n",
        "        combined_nan_mask = np.isnan(composite_flat_masked) | np.isnan(dsm_flat_masked) | np.isnan(tpi_flat_masked) | np.isnan(three_class_flat_masked)\n",
        "\n",
        "\n",
        "        composite_flat_cleaned = composite_flat_masked[~combined_nan_mask]\n",
        "        dsm_flat_cleaned = dsm_flat_masked[~combined_nan_mask]\n",
        "        tpi_flat_cleaned = tpi_flat_masked[~combined_nan_mask]\n",
        "        three_class_flat_cleaned = three_class_flat_masked[~combined_nan_mask]\n",
        "\n",
        "\n",
        "        # 5. Check if there are valid data points left\n",
        "        if composite_flat_cleaned.size == 0:\n",
        "             print(\"No valid data points available after removing NaN values. Skipping analysis.\")\n",
        "        else:\n",
        "            # 6. Create a pandas DataFrame for correlation and scatter plots\n",
        "            analysis_df = pd.DataFrame({\n",
        "                'Composite_Productivity': composite_flat_cleaned,\n",
        "                'DSM': dsm_flat_cleaned,\n",
        "                'TPI': tpi_flat_cleaned,\n",
        "                'Productivity_Class': three_class_flat_cleaned # Include class for zonal stats\n",
        "            })\n",
        "\n",
        "            # 7. Calculate and print the correlation matrix\n",
        "            print(\"\\nCorrelation Matrix:\")\n",
        "            # Calculate correlation only for the numeric columns\n",
        "            correlation_matrix = analysis_df[['Composite_Productivity', 'DSM', 'TPI']].corr()\n",
        "            display(correlation_matrix)\n",
        "\n",
        "            # 8. Generate scatter plots\n",
        "            print(\"\\nGenerating Scatter Plots:\")\n",
        "\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.scatter(analysis_df['DSM'], analysis_df['Composite_Productivity'], alpha=0.5, s=5)\n",
        "            plt.title('Composite Productivity vs. DSM')\n",
        "            plt.xlabel('DSM')\n",
        "            plt.ylabel('Composite Productivity Index')\n",
        "            plt.grid(True)\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.scatter(analysis_df['TPI'], analysis_df['Composite_Productivity'], alpha=0.5, s=5, color='orange')\n",
        "            plt.title('Composite Productivity vs. TPI')\n",
        "            plt.xlabel('TPI')\n",
        "            plt.ylabel('Composite Productivity Index')\n",
        "            plt.grid(True)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # 9. Perform basic zonal statistics analysis\n",
        "            print(\"\\nZonal Statistics (Mean DSM and TPI by Productivity Class):\")\n",
        "            zonal_stats = analysis_df.groupby('Productivity_Class')[['DSM', 'TPI']].mean().reset_index()\n",
        "\n",
        "            # Map class numbers to descriptions for readability\n",
        "            class_mapping = {1: 'Below Average', 2: 'Average', 3: 'Above Average'}\n",
        "            zonal_stats['Productivity_Class'] = zonal_stats['Productivity_Class'].map(class_mapping)\n",
        "\n",
        "            display(zonal_stats)\n",
        "\n",
        "            # 10. Print a summary of the findings\n",
        "            print(\"\\nSummary of Findings:\")\n",
        "            print(\"Based on the analysis:\")\n",
        "            print(\"- Correlation Analysis:\")\n",
        "            print(f\"  - Composite Productivity has a correlation of {correlation_matrix.loc['Composite_Productivity', 'DSM']:.2f} with DSM.\")\n",
        "            print(f\"  - Composite Productivity has a correlation of {correlation_matrix.loc['Composite_Productivity', 'TPI']:.2f} with TPI.\")\n",
        "            print(\"  (Note: Correlation indicates the strength and direction of a linear relationship.)\")\n",
        "            print(\"- Zonal Statistics:\")\n",
        "            print(\"  - The mean DSM and TPI values vary across different productivity classes.\")\n",
        "            for index, row in zonal_stats.iterrows():\n",
        "                print(f\"  - For '{row['Productivity_Class']}' productivity, the mean DSM is {row['DSM']:.2f} and the mean TPI is {row['TPI']:.2f}.\")\n",
        "\n",
        "            # Basic interpretation based on common relationships (can be expanded)\n",
        "            print(\"\\nPotential Interpretation:\")\n",
        "            if correlation_matrix.loc['Composite_Productivity', 'DSM'] > 0:\n",
        "                print(\"- Higher DSM values (generally higher elevations) tend to be associated with higher composite productivity.\")\n",
        "            elif correlation_matrix.loc['Composite_Productivity', 'DSM'] < 0:\n",
        "                print(\"- Lower DSM values (generally lower elevations) tend to be associated with higher composite productivity.\")\n",
        "            else:\n",
        "                print(\"- Little linear relationship observed between Composite Productivity and DSM.\")\n",
        "\n",
        "            if correlation_matrix.loc['Composite_Productivity', 'TPI'] > 0:\n",
        "                print(\"- Higher TPI values (generally ridges or convex areas) tend to be associated with higher composite productivity.\")\n",
        "            elif correlation_matrix.loc['Composite_Productivity', 'TPI'] < 0:\n",
        "                print(\"- Lower TPI values (generally valleys or concave areas) tend to be associated with higher composite productivity.\")\n",
        "            else:\n",
        "                print(\"- Little linear relationship observed between Composite Productivity and TPI.\")\n",
        "\n",
        "            # Additional interpretation based on zonal stats\n",
        "            print(\"\\nConsider the zonal statistics to understand the average topographic characteristics within each productivity class.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required data (composite productivity index, 3-class map, clipped DSM, clipped TPI, or target transform) is not fully available. Skipping relationship analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9db8895f"
      },
      "source": [
        "## Visualize combined data\n",
        "\n",
        "### Subtask:\n",
        "Create visualizations that combine the productivity maps with the topographic data (e.g., overlaying productivity classes on a hillshade of the DSM).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ed6a95"
      },
      "source": [
        "**Reasoning**:\n",
        "Check for the availability of required data and then proceed to create the visualization by generating a hillshade from the DSM, defining a colormap for the 3-class map, and overlaying the 3-class map on the hillshade.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b23a3ad9",
        "outputId": "efb85d55-8aed-44bc-bd29-ef861e42337e"
      },
      "source": [
        "import rasterio.plot\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# 1. Check if required data is available\n",
        "if three_class_map is None or not clipped_dsm_data_and_transforms or 'target_transform' not in locals():\n",
        "    print(\"Required data (3-class map, clipped DSM, or target transform) is not fully available. Skipping visualization.\")\n",
        "else:\n",
        "    print(\"Creating visualization combining productivity map and DSM hillshade...\")\n",
        "\n",
        "    # 2. Extract the clipped DSM array and its transform\n",
        "    # Assuming there is only one clipped DSM raster\n",
        "    clipped_dsm_array, clipped_dsm_transform = clipped_dsm_data_and_transforms[0]\n",
        "\n",
        "    # Ensure the 3-class map and DSM array have consistent shapes\n",
        "    if three_class_map.shape != clipped_dsm_array.shape:\n",
        "        print(f\"Error: 3-class map shape ({three_class_map.shape}) and clipped DSM shape ({clipped_dsm_array.shape}) are inconsistent. Cannot create visualization.\")\n",
        "    else:\n",
        "        # 3. Calculate a hillshade from the clipped DSM array\n",
        "        try:\n",
        "            # Using rasterio's hillshade function\n",
        "            hillshade_array = rasterio.plot.make_hillshade(clipped_dsm_array, dx=clipped_dsm_transform[0], dy=clipped_dsm_transform[4])\n",
        "            print(\"Hillshade calculated from DSM.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating hillshade: {e}\")\n",
        "            hillshade_array = None # Set to None if hillshade calculation fails\n",
        "\n",
        "        if hillshade_array is not None:\n",
        "            # 4. Define a colormap and normalization for the 3-class map\n",
        "            # Define colors for each class (e.g., red for below, yellow for average, green for above)\n",
        "            colors = ['#E41A1C', '#FFC000', '#4DAF4A'] # Red, Amber, Green (colorblind-friendly palette)\n",
        "            class_labels = ['Below Average', 'Average', 'Above Average']\n",
        "            class_values = [1, 2, 3]\n",
        "\n",
        "            cmap = mcolors.ListedColormap(colors)\n",
        "            # Define boundaries for the normalization (midpoints between class values)\n",
        "            bounds = [0.5, 1.5, 2.5, 3.5]\n",
        "            norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "            # Handle NaN values in the 3-class map for plotting\n",
        "            # Create a masked array where NaN values are masked\n",
        "            three_class_map_masked = np.ma.masked_where(np.isnan(three_class_map), three_class_map)\n",
        "\n",
        "            # 5. Create a figure and axes\n",
        "            fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "            # 6. Display the calculated hillshade\n",
        "            rasterio.plot.show(\n",
        "                hillshade_array,\n",
        "                ax=ax,\n",
        "                cmap='gray',\n",
        "                transform=clipped_dsm_transform # Use the transform from the clipped DSM\n",
        "            )\n",
        "            print(\"Hillshade displayed.\")\n",
        "\n",
        "            # 7. Overlay the 3-class map\n",
        "            im = rasterio.plot.show(\n",
        "                three_class_map_masked, # Use the masked array\n",
        "                ax=ax,\n",
        "                cmap=cmap,\n",
        "                norm=norm,\n",
        "                transform=clipped_dsm_transform, # Use the transform from the clipped DSM\n",
        "                alpha=0.6, # Set alpha for semi-transparency\n",
        "                interpolation='nearest' # Use nearest neighbor for discrete classes\n",
        "            )\n",
        "            print(\"3-class map overlaid.\")\n",
        "\n",
        "            # 8. Add a title and labels\n",
        "            ax.set_title('Productivity Classes on DSM Hillshade')\n",
        "            ax.set_xlabel('Easting') # Assuming projected CRS, adjust if necessary\n",
        "            ax.set_ylabel('Northing') # Assuming projected CRS, adjust if necessary\n",
        "\n",
        "            # 9. Add a legend or colorbar\n",
        "            # Create a colorbar with discrete ticks and labels\n",
        "            cbar = fig.colorbar(im.get_images()[0], ax=ax, ticks=class_values)\n",
        "            cbar.ax.set_yticklabels(class_labels)\n",
        "            cbar.set_label('Productivity Class')\n",
        "\n",
        "            # 10. Display the plot\n",
        "            plt.show()\n",
        "            print(\"Visualization displayed.\")\n",
        "        else:\n",
        "            print(\"Skipping visualization due to hillshade calculation error.\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required data (3-class map, clipped DSM, or target transform) is not fully available. Skipping visualization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "127dd638"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The process involved attempting to upload, load, reproject, align, and clip DSM and TPI raster data to analyze their relationship with productivity indices.\n",
        "*   File upload widgets for DSM and TPI were successfully created, but no files were uploaded for either.\n",
        "*   Consequently, the loading of DSM and TPI data resulted in zero datasets being loaded.\n",
        "*   Due to the absence of loaded original datasets, the reprojection and alignment step for DSM and TPI was skipped.\n",
        "*   The clipping of DSM and TPI to the AOI was also skipped because the aligned datasets were not available.\n",
        "*   The analysis of the relationship between productivity indices, DSM, and TPI, including correlation, scatter plots, and zonal statistics, could not be performed due to the lack of clipped DSM and TPI data and potentially other required variables like the composite productivity index and 3-class map.\n",
        "*   The visualization of the combined data (overlaying productivity classes on a DSM hillshade) was also skipped because the necessary clipped DSM and 3-class map were not available.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Ensure that DSM and TPI raster files are successfully uploaded before proceeding with subsequent analysis steps.\n",
        "*   Verify that the composite productivity index and the 3-class productivity map are successfully generated and available from previous steps, as they are crucial inputs for the relationship analysis and visualization.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "feaeed66968742119f8a3b2bc1998c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Skip Alignment",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7ea562d20821420b8746958d37865d06",
            "style": "IPY_MODEL_3d06a8e3820042859bb724bc5a02c35a",
            "value": false
          }
        },
        "7ea562d20821420b8746958d37865d06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d06a8e3820042859bb724bc5a02c35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f4b7505dfaa48d4b550931a399f46a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".kml,.geojson,.shp",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload AOI Polygon (KML, GeoJSON, or SHP)",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_02f4a45ab3964926bac3ca9becc03ab4",
            "metadata": [
              {
                "name": "NW-21-2-14w1FieldBoundary.kml",
                "type": "application/vnd.google-earth.kml+xml",
                "size": 7042,
                "lastModified": 1760799794149
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_543382e49d1148c1a5bf07b68974d895"
          }
        },
        "02f4a45ab3964926bac3ca9becc03ab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "543382e49d1148c1a5bf07b68974d895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "447803c799c1492bb48950a4f42c77d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 14,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".tif,.tiff",
            "button_style": "",
            "data": [
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null,
              null
            ],
            "description": "Upload NDVI Rasters (GeoTIFFs)",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_4b850c07f8c9434f9343ddf37306361a",
            "metadata": [
              {
                "name": "NW-21-002-14W1M_2019-07-25_NDVI.tif",
                "type": "image/tiff",
                "size": 154445,
                "lastModified": 1760798486657
              },
              {
                "name": "NW-21-002-14W1M_2019-07-30_NDVI.tif",
                "type": "image/tiff",
                "size": 154278,
                "lastModified": 1760798486443
              },
              {
                "name": "NW-21-002-14W1M_2019-08-19_NDVI.tif",
                "type": "image/tiff",
                "size": 160379,
                "lastModified": 1760798486313
              },
              {
                "name": "NW-21-002-14W1M_2020-07-14_NDVI.tif",
                "type": "image/tiff",
                "size": 158583,
                "lastModified": 1760798486669
              },
              {
                "name": "NW-21-002-14W1M_2021-07-22_NDVI.tif",
                "type": "image/tiff",
                "size": 157014,
                "lastModified": 1760798486645
              },
              {
                "name": "NW-21-002-14W1M_2021-07-27_NDVI.tif",
                "type": "image/tiff",
                "size": 157894,
                "lastModified": 1760798486374
              },
              {
                "name": "NW-21-002-14W1M_2021-08-08_NDVI.tif",
                "type": "image/tiff",
                "size": 159828,
                "lastModified": 1760798486561
              },
              {
                "name": "NW-21-002-14W1M_2021-08-13_NDVI.tif",
                "type": "image/tiff",
                "size": 160833,
                "lastModified": 1760798486350
              },
              {
                "name": "NW-21-002-14W1M_2022-07-17_NDVI.tif",
                "type": "image/tiff",
                "size": 155616,
                "lastModified": 1760798486550
              },
              {
                "name": "NW-21-002-14W1M_2022-08-08_NDVI.tif",
                "type": "image/tiff",
                "size": 154115,
                "lastModified": 1760798486502
              },
              {
                "name": "NW-21-002-14W1M_2023-07-22_NDVI.tif",
                "type": "image/tiff",
                "size": 154433,
                "lastModified": 1760798486431
              },
              {
                "name": "NW-21-002-14W1M_2024-07-28_NDVI.tif",
                "type": "image/tiff",
                "size": 153692,
                "lastModified": 1760798486421
              },
              {
                "name": "NW-21-002-14W1M_2024-08-02_NDVI.tif",
                "type": "image/tiff",
                "size": 152787,
                "lastModified": 1760798486538
              },
              {
                "name": "NW-21-002-14W1M_2025-07-13_NDVI.tif",
                "type": "image/tiff",
                "size": 157152,
                "lastModified": 1760798486361
              }
            ],
            "multiple": true,
            "style": "IPY_MODEL_8b033930ff1341a49764f9ce97432255"
          }
        },
        "4b850c07f8c9434f9343ddf37306361a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b033930ff1341a49764f9ce97432255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "55492f3e474340baa766b1a502353739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".tif,.tiff",
            "button_style": "",
            "data": [],
            "description": "Upload DSM Rasters (GeoTIFFs)",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_b426b8273b1a4688b3254ae185fc52aa",
            "metadata": [],
            "multiple": true,
            "style": "IPY_MODEL_4c9d5a89a230491fa81d4698352b335b"
          }
        },
        "b426b8273b1a4688b3254ae185fc52aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c9d5a89a230491fa81d4698352b335b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "f33efd3b813a4172864c2bef7110ce7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".tif,.tiff",
            "button_style": "",
            "data": [],
            "description": "Upload TPI Rasters (GeoTIFFs)",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_cb2d16782dd74eee839b03704f1ae6e1",
            "metadata": [],
            "multiple": true,
            "style": "IPY_MODEL_5bd413855a324664941c98cd7ed5bd8d"
          }
        },
        "cb2d16782dd74eee839b03704f1ae6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd413855a324664941c98cd7ed5bd8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}